# Assignment 3 Report

To install dependencies, use 
```bash
pip install -r requirements.txt
```

To run any python3 file, run it from the base directory using,
```bash
python3 -m path.to.code.file # without .py extension
```
To run the code for this assignment,
```bash
python3 -m assignments.3.a3 # (without .py extension)
```
Website references are mentioned in the code.

## 2.1 Data Preprocessing
### 1. Describe the dataset using mean, standard deviation, min, and max values for all attributes


| Attribute            |       Mean |   Standard Deviation |     Min |       Max |
|:---------------------|-----------:|---------------------:|--------:|----------:|
| fixed acidity        |  8.31111   |           1.74683    | 4.6     |  15.9     |
| volatile acidity     |  0.531339  |           0.179555   | 0.12    |   1.58    |
| citric acid          |  0.268364  |           0.1966     | 0       |   1       |
| residual sugar       |  2.53215   |           1.35532    | 0.9     |  15.5     |
| chlorides            |  0.0869326 |           0.0472467  | 0.012   |   0.611   |
| free sulfur dioxide  | 15.6155    |          10.246      | 1       |  68       |
| total sulfur dioxide | 45.9147    |          32.7678     | 6       | 289       |
| density              |  0.99673   |           0.00192422 | 0.99007 |   1.00369 |
| pH                   |  3.31101   |           0.156596   | 2.74    |   4.01    |
| sulphates            |  0.657708  |           0.170324   | 0.33    |   2       |
| alcohol              | 10.4421    |           1.08172    | 8.4     |  14.9     |
| quality              |  5.65704   |           0.805472   | 3       |   8       |

### 2. Draw a graph that shows the distribution of the various labels across the entire dataset. You are allowed to use standard libraries like Matplotlib

![Quality Distribution](./figures/quality_distribution.png)

![Feature Histograms](./figures/distributions.png)

## 3. Normalise and standarize the data. Make sure to handle the missing or inconsistent data values if necessary. You can use sklearn for this.

Saved to data/interim/3/WineQT in WineQT_normalized.csv and WineQT_standardized.csv

## 2.2 Model Building from Scratch 

To initialize the `MLPClassifier`, use the following syntax:

```python
model = MLPClassifier(input_size, hidden_layers, num_classes=6, learning_rate=0.01, activation='sigmoid', optimizer='sgd', wandb_log=False, print_every=10)
```

To fit the model to training data, use the `fit` method:

```python
model.fit(X_train, y_train, X_validation, y_validation, max_epochs=10, batch_size=32, early_stopping=True)
```

To make predictions, use the `predict` method:

```python
predictions = model.predict(X_test)
```

The unit tests for gradient checking in the `TestMLPGradientChecking` class ensure that the gradient computation is accurate for various configurations. All tests pass for the following scenarios:

1. **Sigmoid Activation**: Tests gradient checking using a model with a hidden layer of 10 and 5 neurons, using stochastic gradient descent (SGD).
   ```python
   model.gradient_checking(self.X_train_classification[:20], self.y_train_classification[:20])
   ```

2. **ReLU Activation**: Tests gradient checking with a model having one hidden layer of 10 neurons, using batch gradient descent (BGD).
   ```python
   model.gradient_checking(self.X_train_classification[100:110], self.y_train_classification[100:110])
   ```

3. **Tanh Activation**: Tests gradient checking with a more complex model with three hidden layers (15, 5, and 3 neurons) and using mini-batch gradient descent (MBGD).
   ```python
   model.gradient_checking(self.X_train_classification[200:300], self.y_train_classification[200:300])
   ```

All gradient checks confirm the correctness of the gradients across these configurations.

## 2.3 Model Training & Hyperparameter Tuning using W&B
1. Plot the trend of accuracy scores with change in these hyperparameters
using W&B.
![trend of accuracy scores](figures/2.3.1.png)
![alt text](figures/2.3.2.png)
![alt text](figures/2.3.3.png)

2. Generate a table listing all the hyperparameters tested and their corre-
sponding metrics mentioned above

<details>
  <summary>View Table</summary>


| Activation   |   Batch Size | Hidden Layers   | Optimizer   |   Learning Rate |   Max Epochs |   Validation Accuracy |   Epoch |   F1 Score |   Precision |     Recall |
|:-------------|-------------:|:----------------|:------------|----------------:|-------------:|----------------------:|--------:|-----------:|------------:|-----------:|
| tanh         |           32 | [16]            | mbgd        |           0.002 |          800 |            0.657895   |     800 | 0.290646   |   0.293096  | 0.288235   |
| sigmoid      |           16 | [16]            | mbgd        |           0.01  |          200 |            0.657895   |     200 | 0.311248   |   0.340203  | 0.286835   |
| tanh         |           32 | [16]            | mbgd        |           0.002 |          800 |            0.657895   |     800 | 0.290646   |   0.293096  | 0.288235   |
| sigmoid      |           32 | [16]            | mbgd        |           0.01  |          800 |            0.649123   |     800 | 0.298194   |   0.314412  | 0.283567   |
| sigmoid      |           32 | [16]            | mbgd        |           0.01  |          800 |            0.649123   |     800 | 0.298194   |   0.314412  | 0.283567   |
| tanh         |           32 | [8,16]          | bgd         |           0.01  |          800 |            0.622807   |     800 | 0.29319    |   0.298519  | 0.288049   |
| sigmoid      |           16 | [16,16]         | mbgd        |           0.05  |          800 |            0.622807   |     413 | 0.276794   |   0.283036  | 0.270822   |
| tanh         |           32 | [8,16]          | mbgd        |           0.01  |          200 |            0.622807   |     200 | 0.289318   |   0.299845  | 0.279505   |
| tanh         |           32 | [16]            | mbgd        |           0.002 |          200 |            0.622807   |     200 | 0.281192   |   0.280779  | 0.281606   |
| sigmoid      |           16 | [16]            | mbgd        |           0.01  |          800 |            0.622807   |     800 | 0.265027   |   0.264836  | 0.265219   |
| tanh         |           32 | [16]            | mbgd        |           0.002 |          200 |            0.622807   |     200 | 0.281192   |   0.280779  | 0.281606   |
| sigmoid      |           32 | [8]             | mbgd        |           0.05  |          800 |            0.622807   |     800 | 0.281107   |   0.282728  | 0.279505   |
| tanh         |           32 | [16]            | mbgd        |           0.002 |          200 |            0.622807   |     200 | 0.281192   |   0.280779  | 0.281606   |
| tanh         |           32 | [8,16]          | bgd         |           0.01  |          800 |            0.622807   |     800 | 0.29319    |   0.298519  | 0.288049   |
| relu         |           32 | [8,8,8]         | mbgd        |           0.05  |          200 |            0.622807   |     133 | 0.295498   |   0.29809   | 0.292951   |
| tanh         |           32 | [8,8]           | mbgd        |           0.002 |          200 |            0.614035   |     200 | 0.306628   |   0.366924  | 0.263352   |
| tanh         |           32 | [8,8]           | mbgd        |           0.002 |          200 |            0.614035   |     200 | 0.306628   |   0.366924  | 0.263352   |
| tanh         |           16 | [8,16]          | mbgd        |           0.002 |          200 |            0.614035   |     200 | 0.282699   |   0.287948  | 0.277638   |
| tanh         |           16 | [16]            | mbgd        |           0.002 |          200 |            0.614035   |     200 | 0.266803   |   0.26455   | 0.269094   |
| tanh         |           32 | [8,8]           | mbgd        |           0.002 |          200 |            0.614035   |     200 | 0.306628   |   0.366924  | 0.263352   |
| tanh         |           16 | [8,8]           | mbgd        |           0.01  |          200 |            0.614035   |      93 | 0.283048   |   0.284983  | 0.281139   |
| tanh         |           32 | [8,8]           | mbgd        |           0.01  |          800 |            0.614035   |     356 | 0.291423   |   0.29686   | 0.286181   |
| tanh         |           32 | [8,8]           | mbgd        |           0.002 |          200 |            0.614035   |     200 | 0.306628   |   0.366924  | 0.263352   |
| tanh         |           32 | [8,8]           | mbgd        |           0.01  |          800 |            0.614035   |     356 | 0.291423   |   0.29686   | 0.286181   |
| tanh         |           32 | [8,8]           | mbgd        |           0.002 |          200 |            0.614035   |     200 | 0.306628   |   0.366924  | 0.263352   |
| tanh         |           32 | [8,8]           | mbgd        |           0.002 |          200 |            0.614035   |     200 | 0.306628   |   0.366924  | 0.263352   |
| sigmoid      |           32 | [16]            | mbgd        |           0.002 |          800 |            0.605263   |     800 | 0.219452   |   0.199595  | 0.243697   |
| sigmoid      |           32 | [16]            | mbgd        |           0.002 |          800 |            0.605263   |     800 | 0.219452   |   0.199595  | 0.243697   |
| sigmoid      |           32 | [16]            | mbgd        |           0.002 |          800 |            0.605263   |     800 | 0.219452   |   0.199595  | 0.243697   |
| sigmoid      |           32 | [16]            | mbgd        |           0.002 |          800 |            0.605263   |     800 | 0.219452   |   0.199595  | 0.243697   |
| sigmoid      |           32 | [16]            | mbgd        |           0.002 |          800 |            0.605263   |     800 | 0.219452   |   0.199595  | 0.243697   |
| sigmoid      |           32 | [16]            | mbgd        |           0.002 |          800 |            0.605263   |     800 | 0.219452   |   0.199595  | 0.243697   |
| sigmoid      |           16 | [16]            | mbgd        |           0.002 |          800 |            0.605263   |     800 | 0.264359   |   0.281176  | 0.24944    |
| relu         |           32 | [8,16]          | mbgd        |           0.002 |          200 |            0.605263   |     200 | 0.293064   |   0.323405  | 0.267927   |
| tanh         |           32 | [8,16]          | mbgd        |           0.002 |          200 |            0.605263   |     200 | 0.286946   |   0.292593  | 0.281513   |
| sigmoid      |           32 | [8,8]           | mbgd        |           0.01  |          800 |            0.596491   |     800 | 0.217345   |   0.198785  | 0.239729   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.596491   |     800 | 0.216799   |   0.196928  | 0.24113    |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.596491   |     800 | 0.216799   |   0.196928  | 0.24113    |
| tanh         |           16 | [8,8]           | bgd         |           0.01  |          800 |            0.596491   |     800 | 0.216799   |   0.196928  | 0.24113    |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.596491   |     800 | 0.216799   |   0.196928  | 0.24113    |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.596491   |     800 | 0.216799   |   0.196928  | 0.24113    |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.596491   |     800 | 0.216799   |   0.196928  | 0.24113    |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.596491   |     800 | 0.216799   |   0.196928  | 0.24113    |
| tanh         |           32 | [8,16]          | mbgd        |           0.002 |          800 |            0.596491   |     800 | 0.271448   |   0.273217  | 0.269701   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.596491   |     800 | 0.216799   |   0.196928  | 0.24113    |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.596491   |     800 | 0.216799   |   0.196928  | 0.24113    |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.596491   |     800 | 0.216799   |   0.196928  | 0.24113    |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.596491   |     800 | 0.216799   |   0.196928  | 0.24113    |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.596491   |     800 | 0.216799   |   0.196928  | 0.24113    |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.596491   |     800 | 0.216799   |   0.196928  | 0.24113    |
| sigmoid      |           32 | [8,8]           | mbgd        |           0.01  |          800 |            0.596491   |     800 | 0.217345   |   0.198785  | 0.239729   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.596491   |     800 | 0.216799   |   0.196928  | 0.24113    |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.596491   |     800 | 0.216799   |   0.196928  | 0.24113    |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.596491   |     800 | 0.216799   |   0.196928  | 0.24113    |
| sigmoid      |           32 | [8,8]           | mbgd        |           0.01  |          200 |            0.587719   |     200 | 0.212099   |   0.192754  | 0.235761   |
| tanh         |           16 | [8,8]           | mbgd        |           0.002 |          200 |            0.587719   |     200 | 0.263416   |   0.267675  | 0.25929    |
| sigmoid      |           32 | [8,8]           | mbgd        |           0.01  |          200 |            0.587719   |     200 | 0.212099   |   0.192754  | 0.235761   |
| tanh         |           32 | [8,8]           | mbgd        |           0.002 |          800 |            0.587719   |     800 | 0.263416   |   0.267675  | 0.25929    |
| sigmoid      |           32 | [16,8]          | sgd         |           0.05  |          800 |            0.587719   |     176 | 0.268396   |   0.264061  | 0.272876   |
| sigmoid      |           32 | [8,8]           | mbgd        |           0.01  |          200 |            0.587719   |     200 | 0.212099   |   0.192754  | 0.235761   |
| tanh         |           32 | [8,8]           | mbgd        |           0.002 |          800 |            0.587719   |     800 | 0.263416   |   0.267675  | 0.25929    |
| tanh         |           16 | [8,8]           | mbgd        |           0.002 |          200 |            0.587719   |     200 | 0.263416   |   0.267675  | 0.25929    |
| sigmoid      |           32 | [8,8]           | mbgd        |           0.01  |          200 |            0.587719   |     200 | 0.212099   |   0.192754  | 0.235761   |
| tanh         |           32 | [8,8]           | mbgd        |           0.002 |          800 |            0.587719   |     800 | 0.263416   |   0.267675  | 0.25929    |
| tanh         |           32 | [8,8]           | mbgd        |           0.002 |          800 |            0.587719   |     800 | 0.263416   |   0.267675  | 0.25929    |
| tanh         |           32 | [16]            | bgd         |           0.01  |          800 |            0.587719   |     800 | 0.270887   |   0.274011  | 0.267834   |
| tanh         |           32 | [8,8]           | mbgd        |           0.002 |          800 |            0.587719   |     800 | 0.263416   |   0.267675  | 0.25929    |
| sigmoid      |           16 | [8,8]           | mbgd        |           0.002 |          800 |            0.587719   |     800 | 0.212365   |   0.192726  | 0.236461   |
| tanh         |           32 | [8,8]           | mbgd        |           0.002 |          800 |            0.587719   |     800 | 0.263416   |   0.267675  | 0.25929    |
| tanh         |           16 | [8,8]           | mbgd        |           0.002 |          800 |            0.587719   |     730 | 0.272085   |   0.277981  | 0.266433   |
| sigmoid      |           16 | [8,8]           | mbgd        |           0.01  |          200 |            0.587719   |     200 | 0.212355   |   0.192708  | 0.236461   |
| tanh         |           16 | [8,8]           | mbgd        |           0.002 |          200 |            0.587719   |     200 | 0.263416   |   0.267675  | 0.25929    |
| tanh         |           32 | [8,8]           | mbgd        |           0.002 |          800 |            0.587719   |     800 | 0.263416   |   0.267675  | 0.25929    |
| sigmoid      |           32 | [8,8]           | mbgd        |           0.01  |          200 |            0.587719   |     200 | 0.212099   |   0.192754  | 0.235761   |
| sigmoid      |           32 | [16]            | mbgd        |           0.01  |          200 |            0.578947   |     200 | 0.208525   |   0.189502  | 0.231793   |
| tanh         |           32 | [8,8]           | mbgd        |           0.01  |          200 |            0.578947   |     200 | 0.270427   |   0.276554  | 0.264566   |
| relu         |           16 | [8,8]           | mbgd        |           0.002 |          800 |            0.578947   |     800 | 0.271599   |   0.267949  | 0.27535    |
| sigmoid      |           32 | [16]            | mbgd        |           0.01  |          200 |            0.578947   |     200 | 0.208525   |   0.189502  | 0.231793   |
| tanh         |           32 | [8]             | bgd         |           0.01  |          200 |            0.578947   |     200 | 0.207389   |   0.188095  | 0.231092   |
| signum       |           32 | [8]             | bgd         |           0.01  |          200 |            0.570175   |     200 | 0.20449    |   0.187391  | 0.225023   |
| signum       |           32 | [8]             | bgd         |           0.01  |          200 |            0.570175   |     200 | 0.20449    |   0.187391  | 0.225023   |
| signum       |           32 | [16]            | mbgd        |           0.002 |          200 |            0.570175   |     200 | 0.293244   |   0.287601  | 0.299113   |
| relu         |           16 | [8,16]          | bgd         |           0.01  |          800 |            0.570175   |     800 | 0.271167   |   0.300559  | 0.247012   |
| tanh         |           16 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           16 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           16 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           16 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           16 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| signum       |           32 | [8,16]          | mbgd        |           0.01  |          800 |            0.561404   |     353 | 0.296783   |   0.347916  | 0.258753   |
| tanh         |           16 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           16 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           16 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           16 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           16 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.561404   |     800 | 0.205924   |   0.189153  | 0.225957   |
| tanh         |           16 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           16 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| sigmoid      |           32 | [8,8]           | mbgd        |           0.002 |          800 |            0.552632   |     800 | 0.197636   |   0.179474  | 0.219888   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| sigmoid      |           32 | [8,8]           | mbgd        |           0.002 |          800 |            0.552632   |     800 | 0.197636   |   0.179474  | 0.219888   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           16 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| sigmoid      |           32 | [8,8]           | mbgd        |           0.002 |          800 |            0.552632   |     800 | 0.197636   |   0.179474  | 0.219888   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| signum       |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.552632   |     800 | 0.241709   |   0.244989  | 0.238515   |
| tanh         |           16 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.552632   |     200 | 0.200613   |   0.183472  | 0.221289   |
| tanh         |           32 | [8,16]          | bgd         |           0.01  |          200 |            0.54386    |     200 | 0.261111   |   0.270521  | 0.252334   |
| tanh         |           32 | [8,16]          | bgd         |           0.01  |          200 |            0.54386    |     200 | 0.261111   |   0.270521  | 0.252334   |
| tanh         |           32 | [8,16]          | bgd         |           0.01  |          200 |            0.54386    |     200 | 0.261111   |   0.270521  | 0.252334   |
| tanh         |           32 | [8,16]          | bgd         |           0.01  |          200 |            0.54386    |     200 | 0.261111   |   0.270521  | 0.252334   |
| signum       |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.54386    |     200 | 0.245362   |   0.256386  | 0.235247   |
| tanh         |           32 | [8,16]          | bgd         |           0.01  |          200 |            0.54386    |     200 | 0.261111   |   0.270521  | 0.252334   |
| tanh         |           32 | [8,16]          | bgd         |           0.01  |          200 |            0.54386    |     200 | 0.261111   |   0.270521  | 0.252334   |
| signum       |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.54386    |     800 | 0.245362   |   0.256386  | 0.235247   |
| signum       |           32 | [8,16]          | bgd         |           0.01  |          800 |            0.54386    |     800 | 0.311711   |   0.385872  | 0.261461   |
| signum       |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.54386    |     800 | 0.245362   |   0.256386  | 0.235247   |
| signum       |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.54386    |     800 | 0.245362   |   0.256386  | 0.235247   |
| relu         |           32 | [8,8]           | mbgd        |           0.002 |          200 |            0.535088   |     200 | 0.1922     |   0.174396  | 0.214052   |
| relu         |           32 | [8,16]          | bgd         |           0.01  |          200 |            0.535088   |     200 | 0.273309   |   0.34544   | 0.226097   |
| sigmoid      |           16 | [8,8]           | mbgd        |           0.002 |          200 |            0.526316   |     200 | 0.189045   |   0.172308  | 0.209384   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.517544   |     800 | 0.186394   |   0.169643  | 0.206816   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.517544   |     800 | 0.186394   |   0.169643  | 0.206816   |
| tanh         |           16 | [8,16]          | bgd         |           0.002 |          800 |            0.517544   |     800 | 0.240683   |   0.256321  | 0.226844   |
| sigmoid      |           16 | [8,8]           | bgd         |           0.01  |          800 |            0.517544   |     800 | 0.186394   |   0.169643  | 0.206816   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.517544   |     800 | 0.186394   |   0.169643  | 0.206816   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.517544   |     800 | 0.186394   |   0.169643  | 0.206816   |
| sigmoid      |           16 | [8,8]           | bgd         |           0.01  |          800 |            0.517544   |     800 | 0.186394   |   0.169643  | 0.206816   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.517544   |     800 | 0.186394   |   0.169643  | 0.206816   |
| sigmoid      |           32 | [8]             | bgd         |           0.01  |          200 |            0.517544   |     200 | 0.180779   |   0.164112  | 0.201214   |
| relu         |           16 | [8,8]           | bgd         |           0.01  |          800 |            0.517544   |     800 | 0.185567   |   0.168277  | 0.206816   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.517544   |     800 | 0.186394   |   0.169643  | 0.206816   |
| tanh         |           32 | [8,16]          | bgd         |           0.002 |          800 |            0.517544   |     800 | 0.240683   |   0.256321  | 0.226844   |
| relu         |           16 | [8,16]          | bgd         |           0.002 |          800 |            0.517544   |     800 | 0.191992   |   0.175629  | 0.211718   |
| tanh         |           16 | [8,16]          | bgd         |           0.002 |          800 |            0.517544   |     800 | 0.240683   |   0.256321  | 0.226844   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.517544   |     800 | 0.186394   |   0.169643  | 0.206816   |
| tanh         |           16 | [8,16]          | bgd         |           0.002 |          800 |            0.517544   |     800 | 0.240683   |   0.256321  | 0.226844   |
| tanh         |           32 | [8,16]          | bgd         |           0.002 |          800 |            0.517544   |     800 | 0.240683   |   0.256321  | 0.226844   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.517544   |     800 | 0.186394   |   0.169643  | 0.206816   |
| sigmoid      |           16 | [8]             | bgd         |           0.01  |          200 |            0.517544   |     200 | 0.180779   |   0.164112  | 0.201214   |
| tanh         |           32 | [8,16]          | bgd         |           0.002 |          800 |            0.517544   |     800 | 0.240683   |   0.256321  | 0.226844   |
| tanh         |           32 | [8,16]          | bgd         |           0.002 |          800 |            0.517544   |     800 | 0.240683   |   0.256321  | 0.226844   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.517544   |     800 | 0.186394   |   0.169643  | 0.206816   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.517544   |     800 | 0.186394   |   0.169643  | 0.206816   |
| relu         |           32 | [8,16]          | bgd         |           0.002 |          800 |            0.517544   |     800 | 0.191992   |   0.175629  | 0.211718   |
| relu         |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.517544   |     800 | 0.185567   |   0.168277  | 0.206816   |
| relu         |           32 | [8,16]          | bgd         |           0.002 |          800 |            0.517544   |     800 | 0.191992   |   0.175629  | 0.211718   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.517544   |     800 | 0.186394   |   0.169643  | 0.206816   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.517544   |     800 | 0.186394   |   0.169643  | 0.206816   |
| relu         |           32 | [8,16]          | bgd         |           0.002 |          800 |            0.517544   |     800 | 0.191992   |   0.175629  | 0.211718   |
| sigmoid      |           16 | [8,8]           | bgd         |           0.01  |          800 |            0.517544   |     800 | 0.186394   |   0.169643  | 0.206816   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.517544   |     800 | 0.186394   |   0.169643  | 0.206816   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.517544   |     800 | 0.186394   |   0.169643  | 0.206816   |
| signum       |           16 | [16]            | mbgd        |           0.01  |          800 |            0.517544   |     460 | 0.222849   |   0.222496  | 0.223203   |
| relu         |           32 | [8,16]          | bgd         |           0.002 |          800 |            0.517544   |     800 | 0.191992   |   0.175629  | 0.211718   |
| sigmoid      |           16 | [8,8]           | bgd         |           0.01  |          800 |            0.517544   |     800 | 0.186394   |   0.169643  | 0.206816   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          800 |            0.517544   |     800 | 0.186394   |   0.169643  | 0.206816   |
| tanh         |           16 | [8,16]          | bgd         |           0.002 |          800 |            0.517544   |     800 | 0.240683   |   0.256321  | 0.226844   |
| sigmoid      |           32 | [8,8]           | mbgd        |           0.002 |          200 |            0.5        |     200 | 0.179063   |   0.162838  | 0.19888    |
| sigmoid      |           32 | [8,8]           | mbgd        |           0.002 |          200 |            0.5        |     200 | 0.179063   |   0.162838  | 0.19888    |
| sigmoid      |           32 | [8,8]           | mbgd        |           0.002 |          200 |            0.5        |     200 | 0.179063   |   0.162838  | 0.19888    |
| sigmoid      |           32 | [8,8]           | mbgd        |           0.002 |          200 |            0.5        |     200 | 0.179063   |   0.162838  | 0.19888    |
| sigmoid      |           32 | [8,8]           | mbgd        |           0.002 |          200 |            0.5        |     200 | 0.179063   |   0.162838  | 0.19888    |
| tanh         |           32 | [16]            | bgd         |           0.01  |          200 |            0.473684   |     200 | 0.205972   |   0.207196  | 0.204762   |
| tanh         |           32 | [16]            | bgd         |           0.01  |          200 |            0.473684   |     200 | 0.205972   |   0.207196  | 0.204762   |
| tanh         |           16 | [16]            | bgd         |           0.01  |          200 |            0.473684   |     200 | 0.205972   |   0.207196  | 0.204762   |
| tanh         |           32 | [16]            | bgd         |           0.01  |          200 |            0.473684   |     200 | 0.205972   |   0.207196  | 0.204762   |
| tanh         |           16 | [16]            | bgd         |           0.01  |          200 |            0.473684   |     200 | 0.205972   |   0.207196  | 0.204762   |
| tanh         |           32 | [16]            | bgd         |           0.01  |          200 |            0.473684   |     200 | 0.205972   |   0.207196  | 0.204762   |
| tanh         |           32 | [16]            | bgd         |           0.01  |          200 |            0.473684   |     200 | 0.205972   |   0.207196  | 0.204762   |
| tanh         |           32 | [16]            | bgd         |           0.01  |          200 |            0.473684   |     200 | 0.205972   |   0.207196  | 0.204762   |
| tanh         |           32 | [16]            | bgd         |           0.01  |          200 |            0.473684   |     200 | 0.205972   |   0.207196  | 0.204762   |
| sigmoid      |           16 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           16 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.447368   |     200 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.447368   |     200 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           16 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.447368   |     200 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.447368   |     200 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.447368   |     200 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           16 | [8,16]          | bgd         |           0.002 |          200 |            0.447368   |     200 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           16 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           16 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           16 | [8,16]          | bgd         |           0.002 |          200 |            0.447368   |     200 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           16 | [8,8]           | bgd         |           0.002 |          800 |            0.447368   |     800 | 0.10303    |   0.0745614 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.438596   |     200 | 0.135857   |   0.115909  | 0.164099   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.438596   |     200 | 0.135857   |   0.115909  | 0.164099   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.438596   |     200 | 0.135857   |   0.115909  | 0.164099   |
| sigmoid      |           16 | [8,8]           | bgd         |           0.01  |          200 |            0.438596   |     200 | 0.135857   |   0.115909  | 0.164099   |
| sigmoid      |           16 | [8,8]           | bgd         |           0.01  |          200 |            0.438596   |     200 | 0.135857   |   0.115909  | 0.164099   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.438596   |     200 | 0.135857   |   0.115909  | 0.164099   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.438596   |     200 | 0.135857   |   0.115909  | 0.164099   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.438596   |     200 | 0.135857   |   0.115909  | 0.164099   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.438596   |     200 | 0.135857   |   0.115909  | 0.164099   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.438596   |     200 | 0.135857   |   0.115909  | 0.164099   |
| sigmoid      |           32 | [8,16]          | mbgd        |           0.002 |          200 |            0.429825   |     200 | 0.152469   |   0.138725  | 0.169234   |
| sigmoid      |           32 | [8,16]          | mbgd        |           0.002 |          200 |            0.429825   |     200 | 0.152469   |   0.138725  | 0.169234   |
| sigmoid      |           32 | [16]            | mbgd        |           0.002 |          200 |            0.429825   |     200 | 0.156153   |   0.142003  | 0.173436   |
| sigmoid      |           32 | [8,16]          | mbgd        |           0.002 |          200 |            0.429825   |     200 | 0.152469   |   0.138725  | 0.169234   |
| relu         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.421053   |     800 | 0.152328   |   0.138337  | 0.169468   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          800 |            0.421053   |     800 | 0.156202   |   0.145379  | 0.168768   |
| tanh         |           16 | [16]            | bgd         |           0.002 |          800 |            0.421053   |     800 | 0.156202   |   0.145379  | 0.168768   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          800 |            0.421053   |     800 | 0.156202   |   0.145379  | 0.168768   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          800 |            0.421053   |     800 | 0.156202   |   0.145379  | 0.168768   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          800 |            0.421053   |     800 | 0.156202   |   0.145379  | 0.168768   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          800 |            0.421053   |     800 | 0.156202   |   0.145379  | 0.168768   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          800 |            0.421053   |     800 | 0.156202   |   0.145379  | 0.168768   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          800 |            0.421053   |     800 | 0.156202   |   0.145379  | 0.168768   |
| tanh         |           16 | [16]            | bgd         |           0.002 |          800 |            0.421053   |     800 | 0.156202   |   0.145379  | 0.168768   |
| relu         |           16 | [8,8]           | bgd         |           0.002 |          800 |            0.421053   |     800 | 0.152328   |   0.138337  | 0.169468   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          800 |            0.421053   |     800 | 0.156202   |   0.145379  | 0.168768   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          800 |            0.421053   |     800 | 0.156202   |   0.145379  | 0.168768   |
| relu         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.421053   |     800 | 0.152328   |   0.138337  | 0.169468   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          800 |            0.421053   |     800 | 0.156202   |   0.145379  | 0.168768   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          800 |            0.421053   |     800 | 0.156202   |   0.145379  | 0.168768   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          800 |            0.421053   |     800 | 0.156202   |   0.145379  | 0.168768   |
| relu         |           32 | [8,8]           | bgd         |           0.002 |          800 |            0.421053   |     800 | 0.152328   |   0.138337  | 0.169468   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          800 |            0.421053   |     800 | 0.156202   |   0.145379  | 0.168768   |
| sigmoid      |           32 | [16,16]         | bgd         |           0.05  |          800 |            0.412281   |     800 | 0.149777   |   0.137265  | 0.164799   |
| relu         |           16 | [8]             | bgd         |           0.002 |          200 |            0.403509   |     200 | 0.111922   |   0.0891473 | 0.150327   |
| relu         |           16 | [8,8]           | bgd         |           0.01  |          200 |            0.403509   |     200 | 0.147248   |   0.133847  | 0.163632   |
| relu         |           32 | [8,8]           | bgd         |           0.01  |          200 |            0.403509   |     200 | 0.147248   |   0.133847  | 0.163632   |
| relu         |           32 | [16,8]          | bgd         |           0.002 |          800 |            0.394737   |     800 | 0.132294   |   0.117981  | 0.15056    |
| relu         |           32 | [16]            | bgd         |           0.002 |          800 |            0.394737   |     800 | 0.175415   |   0.180679  | 0.170448   |
| sigmoid      |           32 | [16]            | bgd         |           0.01  |          800 |            0.385965   |     800 | 0.141125   |   0.128102  | 0.157096   |
| sigmoid      |           32 | [8,16]          | bgd         |           0.002 |          800 |            0.385965   |     800 | 0.119349   |   0.100978  | 0.145892   |
| sigmoid      |           32 | [8,16]          | bgd         |           0.002 |          800 |            0.385965   |     800 | 0.119349   |   0.100978  | 0.145892   |
| sigmoid      |           32 | [8,16]          | bgd         |           0.002 |          800 |            0.385965   |     800 | 0.119349   |   0.100978  | 0.145892   |
| sigmoid      |           32 | [16]            | bgd         |           0.01  |          800 |            0.385965   |     800 | 0.141125   |   0.128102  | 0.157096   |
| sigmoid      |           32 | [16]            | bgd         |           0.01  |          800 |            0.385965   |     800 | 0.141125   |   0.128102  | 0.157096   |
| sigmoid      |           16 | [16]            | bgd         |           0.01  |          800 |            0.385965   |     800 | 0.141125   |   0.128102  | 0.157096   |
| sigmoid      |           32 | [16]            | bgd         |           0.01  |          800 |            0.385965   |     800 | 0.141125   |   0.128102  | 0.157096   |
| sigmoid      |           32 | [16]            | bgd         |           0.01  |          800 |            0.385965   |     800 | 0.141125   |   0.128102  | 0.157096   |
| sigmoid      |           32 | [16]            | bgd         |           0.01  |          800 |            0.385965   |     800 | 0.141125   |   0.128102  | 0.157096   |
| sigmoid      |           32 | [16]            | bgd         |           0.01  |          800 |            0.385965   |     800 | 0.141125   |   0.128102  | 0.157096   |
| sigmoid      |           32 | [16]            | bgd         |           0.01  |          800 |            0.385965   |     800 | 0.141125   |   0.128102  | 0.157096   |
| sigmoid      |           32 | [8,16]          | bgd         |           0.002 |          800 |            0.385965   |     800 | 0.119349   |   0.100978  | 0.145892   |
| sigmoid      |           32 | [16]            | bgd         |           0.01  |          800 |            0.385965   |     800 | 0.141125   |   0.128102  | 0.157096   |
| sigmoid      |           32 | [8,16]          | bgd         |           0.002 |          800 |            0.385965   |     800 | 0.119349   |   0.100978  | 0.145892   |
| sigmoid      |           32 | [16]            | bgd         |           0.01  |          800 |            0.385965   |     800 | 0.141125   |   0.128102  | 0.157096   |
| relu         |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.153907   |   0.148148  | 0.160131   |
| tanh         |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.160581   |   0.168627  | 0.153268   |
| tanh         |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.160581   |   0.168627  | 0.153268   |
| tanh         |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.160581   |   0.168627  | 0.153268   |
| relu         |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.153907   |   0.148148  | 0.160131   |
| tanh         |           16 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.160581   |   0.168627  | 0.153268   |
| tanh         |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.160581   |   0.168627  | 0.153268   |
| tanh         |           16 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.160581   |   0.168627  | 0.153268   |
| tanh         |           16 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.160581   |   0.168627  | 0.153268   |
| sigmoid      |           32 | [8,16]          | bgd         |           0.01  |          800 |            0.377193   |     800 | 0.131372   |   0.118406  | 0.147526   |
| tanh         |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.160581   |   0.168627  | 0.153268   |
| tanh         |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.160581   |   0.168627  | 0.153268   |
| tanh         |           16 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.160581   |   0.168627  | 0.153268   |
| relu         |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.153907   |   0.148148  | 0.160131   |
| tanh         |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.160581   |   0.168627  | 0.153268   |
| relu         |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.153907   |   0.148148  | 0.160131   |
| tanh         |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.160581   |   0.168627  | 0.153268   |
| tanh         |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.160581   |   0.168627  | 0.153268   |
| relu         |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.153907   |   0.148148  | 0.160131   |
| relu         |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.153907   |   0.148148  | 0.160131   |
| tanh         |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.160581   |   0.168627  | 0.153268   |
| tanh         |           16 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.160581   |   0.168627  | 0.153268   |
| tanh         |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.160581   |   0.168627  | 0.153268   |
| tanh         |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.160581   |   0.168627  | 0.153268   |
| relu         |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.153907   |   0.148148  | 0.160131   |
| tanh         |           16 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.160581   |   0.168627  | 0.153268   |
| tanh         |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.160581   |   0.168627  | 0.153268   |
| tanh         |           16 | [8,16]          | bgd         |           0.002 |          200 |            0.377193   |     200 | 0.160581   |   0.168627  | 0.153268   |
| sigmoid      |           32 | [16]            | bgd         |           0.01  |          200 |            0.368421   |     200 | 0.136635   |   0.120614  | 0.157563   |
| sigmoid      |           16 | [16]            | bgd         |           0.01  |          200 |            0.368421   |     200 | 0.136635   |   0.120614  | 0.157563   |
| relu         |           16 | [8,8]           | bgd         |           0.002 |          200 |            0.368421   |     200 | 0.175328   |   0.162497  | 0.190359   |
| sigmoid      |           16 | [16]            | bgd         |           0.01  |          200 |            0.368421   |     200 | 0.136635   |   0.120614  | 0.157563   |
| relu         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.368421   |     200 | 0.175328   |   0.162497  | 0.190359   |
| sigmoid      |           16 | [16]            | bgd         |           0.01  |          200 |            0.368421   |     200 | 0.136635   |   0.120614  | 0.157563   |
| relu         |           16 | [8,8]           | bgd         |           0.002 |          200 |            0.368421   |     200 | 0.175328   |   0.162497  | 0.190359   |
| relu         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.368421   |     200 | 0.175328   |   0.162497  | 0.190359   |
| sigmoid      |           32 | [16]            | bgd         |           0.01  |          200 |            0.368421   |     200 | 0.136635   |   0.120614  | 0.157563   |
| relu         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.368421   |     200 | 0.175328   |   0.162497  | 0.190359   |
| relu         |           16 | [8,8]           | bgd         |           0.002 |          200 |            0.368421   |     200 | 0.175328   |   0.162497  | 0.190359   |
| sigmoid      |           32 | [16]            | bgd         |           0.01  |          200 |            0.368421   |     200 | 0.136635   |   0.120614  | 0.157563   |
| relu         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.368421   |     200 | 0.175328   |   0.162497  | 0.190359   |
| sigmoid      |           32 | [16]            | bgd         |           0.01  |          200 |            0.368421   |     200 | 0.136635   |   0.120614  | 0.157563   |
| tanh         |           16 | [16,8]          | bgd         |           0.01  |          200 |            0.359649   |     200 | 0.169991   |   0.172008  | 0.168021   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          200 |            0.350877   |     200 | 0.133505   |   0.125483  | 0.142624   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          200 |            0.350877   |     200 | 0.133505   |   0.125483  | 0.142624   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          200 |            0.350877   |     200 | 0.133505   |   0.125483  | 0.142624   |
| tanh         |           16 | [16]            | bgd         |           0.002 |          200 |            0.350877   |     200 | 0.133505   |   0.125483  | 0.142624   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          200 |            0.350877   |     200 | 0.133505   |   0.125483  | 0.142624   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          200 |            0.350877   |     200 | 0.133505   |   0.125483  | 0.142624   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          200 |            0.350877   |     200 | 0.133505   |   0.125483  | 0.142624   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          200 |            0.350877   |     200 | 0.133505   |   0.125483  | 0.142624   |
| tanh         |           16 | [16]            | bgd         |           0.002 |          200 |            0.350877   |     200 | 0.133505   |   0.125483  | 0.142624   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          200 |            0.350877   |     200 | 0.133505   |   0.125483  | 0.142624   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          200 |            0.350877   |     200 | 0.133505   |   0.125483  | 0.142624   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          200 |            0.350877   |     200 | 0.133505   |   0.125483  | 0.142624   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          200 |            0.350877   |     200 | 0.133505   |   0.125483  | 0.142624   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          200 |            0.350877   |     200 | 0.133505   |   0.125483  | 0.142624   |
| tanh         |           16 | [16]            | bgd         |           0.002 |          200 |            0.350877   |     200 | 0.133505   |   0.125483  | 0.142624   |
| tanh         |           16 | [16]            | bgd         |           0.002 |          200 |            0.350877   |     200 | 0.133505   |   0.125483  | 0.142624   |
| tanh         |           32 | [16]            | bgd         |           0.002 |          200 |            0.350877   |     200 | 0.133505   |   0.125483  | 0.142624   |
| tanh         |           32 | [16,8]          | bgd         |           0.002 |          800 |            0.342105   |     800 | 0.174206   |   0.178595  | 0.170028   |
| tanh         |           16 | [16,8]          | bgd         |           0.002 |          800 |            0.342105   |     800 | 0.174206   |   0.178595  | 0.170028   |
| signum       |           32 | [8,16]          | bgd         |           0.01  |          200 |            0.333333   |     200 | 0.123314   |   0.114217  | 0.133987   |
| signum       |           32 | [8,16]          | bgd         |           0.002 |          800 |            0.333333   |     800 | 0.123314   |   0.114217  | 0.133987   |
| signum       |           32 | [8,16]          | bgd         |           0.002 |          800 |            0.333333   |     800 | 0.123314   |   0.114217  | 0.133987   |
| signum       |           32 | [8,16]          | bgd         |           0.01  |          200 |            0.333333   |     200 | 0.123314   |   0.114217  | 0.133987   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          800 |            0.315789   |     800 | 0.135276   |   0.124638  | 0.147899   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          800 |            0.315789   |     800 | 0.135276   |   0.124638  | 0.147899   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          800 |            0.315789   |     800 | 0.135276   |   0.124638  | 0.147899   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          800 |            0.315789   |     800 | 0.135276   |   0.124638  | 0.147899   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          800 |            0.315789   |     800 | 0.135276   |   0.124638  | 0.147899   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          800 |            0.315789   |     800 | 0.135276   |   0.124638  | 0.147899   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          800 |            0.315789   |     800 | 0.135276   |   0.124638  | 0.147899   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          800 |            0.315789   |     800 | 0.135276   |   0.124638  | 0.147899   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          800 |            0.315789   |     800 | 0.135276   |   0.124638  | 0.147899   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          800 |            0.315789   |     800 | 0.135276   |   0.124638  | 0.147899   |
| sigmoid      |           16 | [16]            | bgd         |           0.002 |          800 |            0.315789   |     800 | 0.135276   |   0.124638  | 0.147899   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          800 |            0.315789   |     800 | 0.135276   |   0.124638  | 0.147899   |
| signum       |           32 | [16]            | bgd         |           0.01  |          200 |            0.307018   |     200 | 0.166547   |   0.186403  | 0.150514   |
| signum       |           32 | [16]            | bgd         |           0.01  |          200 |            0.307018   |     200 | 0.166547   |   0.186403  | 0.150514   |
| signum       |           32 | [16,8]          | bgd         |           0.01  |          200 |            0.27193    |     200 | 0.139843   |   0.166861  | 0.120355   |
| signum       |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.254386   |     200 | 0.137287   |   0.141667  | 0.13317    |
| signum       |           32 | [8,16]          | bgd         |           0.002 |          200 |            0.254386   |     200 | 0.137287   |   0.141667  | 0.13317    |
| signum       |           32 | [16]            | bgd         |           0.002 |          200 |            0.245614   |     200 | 0.18114    |   0.15874   | 0.210901   |
| signum       |           32 | [16]            | bgd         |           0.002 |          200 |            0.245614   |     200 | 0.18114    |   0.15874   | 0.210901   |
| signum       |           32 | [16]            | bgd         |           0.002 |          200 |            0.245614   |     200 | 0.18114    |   0.15874   | 0.210901   |
| signum       |           32 | [16]            | bgd         |           0.002 |          200 |            0.245614   |     200 | 0.18114    |   0.15874   | 0.210901   |
| tanh         |           32 | [16,8]          | bgd         |           0.002 |          200 |            0.236842   |     200 | 0.152424   |   0.101538  | 0.305555   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          200 |            0.140351   |     200 | 0.0726682  |   0.0419974 | 0.269444   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          200 |            0.140351   |     200 | 0.0726682  |   0.0419974 | 0.269444   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          200 |            0.140351   |     200 | 0.0726682  |   0.0419974 | 0.269444   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          200 |            0.140351   |     200 | 0.0726682  |   0.0419974 | 0.269444   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          200 |            0.140351   |     200 | 0.0726682  |   0.0419974 | 0.269444   |
| sigmoid      |           16 | [16]            | bgd         |           0.002 |          200 |            0.140351   |     200 | 0.0726682  |   0.0419974 | 0.269444   |
| sigmoid      |           16 | [16]            | bgd         |           0.002 |          200 |            0.140351   |     200 | 0.0726682  |   0.0419974 | 0.269444   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          200 |            0.140351   |     200 | 0.0726682  |   0.0419974 | 0.269444   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          200 |            0.140351   |     200 | 0.0726682  |   0.0419974 | 0.269444   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          200 |            0.140351   |     200 | 0.0726682  |   0.0419974 | 0.269444   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          200 |            0.140351   |     200 | 0.0726682  |   0.0419974 | 0.269444   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          200 |            0.140351   |     200 | 0.0726682  |   0.0419974 | 0.269444   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          200 |            0.140351   |     200 | 0.0726682  |   0.0419974 | 0.269444   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          200 |            0.140351   |     200 | 0.0726682  |   0.0419974 | 0.269444   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          200 |            0.140351   |     200 | 0.0726682  |   0.0419974 | 0.269444   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          200 |            0.140351   |     200 | 0.0726682  |   0.0419974 | 0.269444   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          200 |            0.140351   |     200 | 0.0726682  |   0.0419974 | 0.269444   |
| sigmoid      |           32 | [16]            | bgd         |           0.002 |          200 |            0.140351   |     200 | 0.0726682  |   0.0419974 | 0.269444   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           16 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           16 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           16 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| sigmoid      |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.131579   |     200 | 0.0387597  |   0.0219298 | 0.166667   |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           16 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           16 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           16 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           16 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           16 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           16 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           16 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| tanh         |           16 | [8,8]           | bgd         |           0.002 |          200 |            0.114035   |     200 | 0.0682109  |   0.0423611 | 0.175      |
| relu         |           32 | [16]            | bgd         |           0.002 |          200 |            0.105263   |     200 | 0.111236   |   0.0747677 | 0.217157   |
| relu         |           16 | [16]            | bgd         |           0.002 |          200 |            0.105263   |     200 | 0.111236   |   0.0747677 | 0.217157   |
| signum       |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.0526316  |     200 | 0.0212766  |   0.0126582 | 0.0666667  |
| signum       |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.0526316  |     200 | 0.0212766  |   0.0126582 | 0.0666667  |
| signum       |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.0526316  |     200 | 0.0212766  |   0.0126582 | 0.0666667  |
| signum       |           32 | [8,8]           | bgd         |           0.002 |          200 |            0.0526316  |     200 | 0.0212766  |   0.0126582 | 0.0666667  |
| signum       |           32 | [8]             | bgd         |           0.002 |          200 |            0.00877193 |     200 | 0.00628931 |   0.0833333 | 0.00326797 |
| signum       |           32 | [8]             | bgd         |           0.002 |          200 |            0.00877193 |     200 | 0.00628931 |   0.0833333 | 0.00326797 |

</details>



3. Report the parameters for the best model that you get

| Parameter                   | Value      |
|-----------------------------|------------|
| Activation                  | tanh       |
| Batch Size                  | 32         |
| Number of Hidden Layers     | 1          |
| Neurons in Hidden Layer     | 16         |
| Learning Rate               | 0.002      |
| Max Epochs                  | 800        |
| Optimizer                   | mbgd       |

## 2.4 Evaluating Single-label Classification Model

<details>
  <summary>Best model run </summary>

| Epoch | Train Loss | Train Accuracy | Validation Accuracy |
|-------|------------|----------------|---------------------|
| 0     | 3.0785     | 0.0503         | 0.0789              |
| 10    | 1.4663     | 0.3326         | 0.3596              |
| 20    | 1.3597     | 0.3479         | 0.3947              |
| 30    | 1.3117     | 0.3600         | 0.4386              |
| 40    | 1.2778     | 0.3742         | 0.4912              |
| 50    | 1.2510     | 0.3829         | 0.5175              |
| 60    | 1.2286     | 0.3928         | 0.5175              |
| 70    | 1.2094     | 0.4114         | 0.5175              |
| 80    | 1.1926     | 0.4245         | 0.5351              |
| 90    | 1.1775     | 0.4464         | 0.5351              |
| 100   | 1.1640     | 0.4540         | 0.5526              |
| 110   | 1.1518     | 0.4573         | 0.5614              |
| 120   | 1.1407     | 0.4705         | 0.5614              |
| 130   | 1.1305     | 0.4858         | 0.5614              |
| 140   | 1.1211     | 0.4967         | 0.5877              |
| 150   | 1.1125     | 0.5120         | 0.5789              |
| 160   | 1.1045     | 0.5197         | 0.5965              |
| 170   | 1.0970     | 0.5274         | 0.5965              |
| 180   | 1.0901     | 0.5306         | 0.5965              |
| 190   | 1.0837     | 0.5361         | 0.5965              |
| 200   | 1.0777     | 0.5427         | 0.6228              |
| 210   | 1.0721     | 0.5470         | 0.6228              |
| 220   | 1.0669     | 0.5503         | 0.6140              |
| 230   | 1.0621     | 0.5481         | 0.6228              |
| 240   | 1.0575     | 0.5492         | 0.6228              |
| 250   | 1.0532     | 0.5569         | 0.6053              |
| 260   | 1.0492     | 0.5624         | 0.6140              |
| 270   | 1.0454     | 0.5635         | 0.6228              |
| 280   | 1.0418     | 0.5689         | 0.6228              |
| 290   | 1.0384     | 0.5700         | 0.6140              |
| 300   | 1.0352     | 0.5689         | 0.6140              |
| 310   | 1.0322     | 0.5700         | 0.5965              |
| 320   | 1.0293     | 0.5755         | 0.6053              |
| 330   | 1.0266     | 0.5766         | 0.6053              |
| 340   | 1.0240     | 0.5799         | 0.6140              |
| 350   | 1.0215     | 0.5832         | 0.6140              |
| 360   | 1.0192     | 0.5832         | 0.6140              |
| 370   | 1.0169     | 0.5821         | 0.6140              |
| 380   | 1.0148     | 0.5842         | 0.6140              |
| 390   | 1.0127     | 0.5853         | 0.6140              |
| 400   | 1.0107     | 0.5886         | 0.6140              |
| 410   | 1.0088     | 0.5897         | 0.6140              |
| 420   | 1.0070     | 0.5886         | 0.6140              |
| 430   | 1.0053     | 0.5897         | 0.6140              |
| 440   | 1.0036     | 0.5886         | 0.6140              |
| 450   | 1.0020     | 0.5908         | 0.6140              |
| 460   | 1.0004     | 0.5875         | 0.6140              |
| 470   | 0.9989     | 0.5875         | 0.6053              |
| 480   | 0.9975     | 0.5908         | 0.6316              |
| 490   | 0.9961     | 0.5908         | 0.6316              |
| 500   | 0.9947     | 0.5930         | 0.6228              |
| 510   | 0.9934     | 0.5886         | 0.6316              |
| 520   | 0.9921     | 0.5941         | 0.6316              |
| 530   | 0.9909     | 0.5941         | 0.6316              |
| 540   | 0.9897     | 0.5952         | 0.6316              |
| 550   | 0.9885     | 0.5963         | 0.6316              |
| 560   | 0.9874     | 0.5952         | 0.6316              |
| 570   | 0.9863     | 0.5952         | 0.6404              |
| 580   | 0.9852     | 0.5941         | 0.6491              |
| 590   | 0.9842     | 0.5930         | 0.6491              |
| 600   | 0.9832     | 0.5930         | 0.6491              |
| 610   | 0.9822     | 0.5963         | 0.6491              |
| 620   | 0.9812     | 0.5974         | 0.6491              |
| 630   | 0.9803     | 0.5985         | 0.6491              |
| 640   | 0.9793     | 0.5996         | 0.6579              |
| 650   | 0.9784     | 0.5996         | 0.6491              |
| 660   | 0.9775     | 0.6039         | 0.6579              |
| 670   | 0.9767     | 0.6039         | 0.6491              |
| 680   | 0.9758     | 0.6039         | 0.6491              |
| 690   | 0.9750     | 0.6061         | 0.6491              |
| 700   | 0.9742     | 0.6061         | 0.6491              |
| 710   | 0.9734     | 0.6072         | 0.6491              |
| 720   | 0.9726     | 0.6061         | 0.6491              |
| 730   | 0.9719     | 0.6061         | 0.6491              |
| 740   | 0.9711     | 0.6072         | 0.6491              |
| 750   | 0.9704     | 0.6072         | 0.6491              |
| 760   | 0.9696     | 0.6083         | 0.6491              |
| 770   | 0.9689     | 0.6094         | 0.6491              |
| 780   | 0.9683     | 0.6061         | 0.6579              |
| 790   | 0.9676     | 0.6061         | 0.6491              |
| 799   | 0.9670     | 0.6072         | 0.6579              |

</details>

### Performance Metrics

| Metric        | Value                          |
|---------------|--------------------------------|
| Accuracy      | 0.6521739130434783             |
| Precision     | 0.31814371501020755            |
| Recall        | 0.29970319420530983            |
| F1 Score      | 0.3086482628066152             |

## 2.5 Analyzing Hyperparameters Effects

![Plot](./figures/2.5.1.png)
![Plot](./figures/2.5.2.png)
![Plot](./figures/2.5.3.png)

### 1. Effect of Non-linearity (Activation Functions):
   **(Activation functions tested: Sigmoid, ReLU, Tanh, Linear)**
   
   - **Sigmoid and Tanh**: These two functions show very similar patterns. Both achieve faster convergence in the initial epochs but plateau around a loss of 1.0 after around 100 epochs. The "S-shaped" curve characteristic of sigmoid and tanh helps in non-linearity, allowing the model to adjust weights efficiently in the beginning, but both suffer from diminishing gradients as training progresses, leading to slower updates later.
   - **ReLU**: This function shows slower initial convergence compared to Tanh and Sigmoid, as seen in the higher loss at the start. However, ReLU tends to achieve a lower overall loss after about 200 epochs. This indicates that ReLU avoids the vanishing gradient problem more effectively, especially over longer training sessions.
   - **Linear**: Although usually not ideal for deep networks because they lack the ability to capture non-linear patterns, essential for most complex tasks. for this dataset, the linear activation performs comparably to other non-linear functions

   **Observation**: Non-linear activations (Sigmoid, Tanh, ReLU) lead to faster and more effective learning. ReLU appears to perform better over extended training.

### 2. Effect of Learning Rate:
   **(Learning rates tested: 0.001, 0.005, 0.01, 0.05)**
   
   - **LR = 0.001**: Shows the slowest convergence, with a gradual decrease in loss. The model does not fully converge even after 800 epochs, indicating that a lower learning rate results in more stable but slower learning.
   - **LR = 0.005 and 0.01**: Both of these learning rates show similar behavior. They converge more quickly compared to 0.001 and reach a minimum loss earlier (around 200-300 epochs). However, LR = 0.01 converges slightly faster and to a lower loss than LR = 0.005.
   - **LR = 0.05**: This learning rate converges very quickly, but there is more volatility in the loss curve, which suggests that the model is overshooting the optimal solution. After around 100 epochs, the loss instability leads to early stopping. 
   **Observation**: Higher learning rates (like 0.05) converge faster but may lead to instability and overshooting. Moderate learning rates (0.01) seem to offer the best balance of convergence speed and stability. A very low learning rate (0.001) results in slow convergence, requiring significantly more epochs to reach optimal performance.

### 3. Effect of Batch Size:
   **(Batch sizes tested: 16, 32, 64, 128)**
   
   - **Batch size 16**: The smallest batch size converges the fastest, as smaller batches allow more frequent updates to the weights. However, the smaller batch size also results in noisier convergence, with fluctuations in the loss curve during the early stages of training.
   - **Batch size 32**: Offers a balanced performance, converging quickly with more stability than batch size 16. The model reaches a low loss with less noise compared to batch size 16, making it a good compromise between frequency of updates and stability.
   - **Batch size 64**: Slightly slower convergence than batch size 32, but the curve is more stable with fewer fluctuations. The larger batch size reduces the variance in weight updates, contributing to more stable but slower learning.
   - **Batch size 128**: This results in the slowest convergence. While the loss curve is the smoothest, it takes much longer to reach a similar loss compared to smaller batch sizes. The reduced number of updates per epoch with this larger batch size contributes to this slower convergence.

   **Observation**: Smaller batch sizes (16, 32) lead to faster convergence but with more noise, whereas larger batch sizes (64, 128) smooth out the learning but slow down the convergence. Batch size 32 appears to be the best compromise between convergence speed and stability.

## 2.6 Multi-Label Classification

![Plot](./figures/2.6.1.png)

<details>
  <summary>View Table</summary>

| Activation   |   Batch Size | Hidden Layers   | Optimizer   |   Learning Rate |   Max Epochs |   Validation Accuracy |   Epoch |   F1 Score |   Precision |   Recall |   Hamming Accuracy |   Hamming Loss |
|:-------------|-------------:|:----------------|:------------|----------------:|-------------:|----------------------:|--------:|-----------:|------------:|---------:|-------------------:|---------------:|
| sigmoid      |           16 | [128,64]        | sgd         |            0.1  |         1500 |                  0.11 |    1500 |   0.692005 |    0.727483 | 0.659827 |            0.73875 |        0.26125 |
| sigmoid      |           32 | [128,64]        | sgd         |            0.01 |         1500 |                  0.06 |    1500 |   0.593874 |    0.618561 | 0.571082 |            0.67    |        0.33    |
| relu         |           16 | [128]           | mbgd        |            0.05 |         1500 |                  0.05 |    1500 |   0.664255 |    0.663834 | 0.664675 |            0.69625 |        0.30375 |
| tanh         |           64 | [64,64]         | sgd         |            0.01 |         1500 |                  0.05 |    1500 |   0.552861 |    0.553342 | 0.552381 |            0.6     |        0.4     |
| relu         |           64 | [128,64]        | bgd         |            0.2  |         1500 |                  0.02 |     217 |   0.563987 |    0.583624 | 0.545628 |            0.6525  |        0.3475  |
| tanh         |           64 | [64,64,32]      | sgd         |            0.2  |         1500 |                  0.01 |     236 |   0.539263 |    0.540961 | 0.537576 |            0.59875 |        0.40125 |
| sigmoid      |           16 | [64,64,32]      | sgd         |            0.05 |         1500 |                  0.01 |    1500 |   0.617054 |    0.656833 | 0.581818 |            0.6875  |        0.3125  |
| relu         |           64 | [64,32]         | bgd         |            0.2  |         1500 |                  0.01 |     249 |   0.542047 |    0.580128 | 0.508658 |            0.65625 |        0.34375 |
| relu         |           64 | [64,32]         | bgd         |            0.2  |         1500 |                  0.01 |     137 |   0.506441 |    0.510522 | 0.502424 |            0.64125 |        0.35875 |
| relu         |           64 | [128,64]        | mbgd        |            0.2  |         1500 |                  0.01 |     214 |   0.533115 |    0.549813 | 0.517403 |            0.645   |        0.355   |
| sigmoid      |           16 | [64,32]         | sgd         |            0.01 |         1500 |                  0.01 |    1500 |   0.562842 |    0.605957 | 0.525455 |            0.66125 |        0.33875 |
| sigmoid      |           16 | [64,64,32]      | sgd         |            0.01 |         1500 |                  0.01 |    1500 |   0.528458 |    0.55617  | 0.503377 |            0.655   |        0.345   |
| relu         |           64 | [64,64,32]      | mbgd        |            0.1  |         1500 |                  0    |     198 |   0.460169 |    0.427673 | 0.498009 |            0.6525  |        0.3475  |
| relu         |           16 | [64,64,32]      | bgd         |            0.05 |         1500 |                  0    |     294 |   0.523973 |    0.538203 | 0.510476 |            0.645   |        0.355   |
| relu         |           64 | [128,64]        | mbgd        |            0.2  |         1500 |                  0    |     143 |   0.497065 |    0.494672 | 0.499481 |            0.64875 |        0.35125 |
| relu         |           64 | [64,64,32]      | mbgd        |            0.2  |         1500 |                  0    |     169 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,64,32]      | mbgd        |            0.2  |         1500 |                  0    |     155 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [128,64]        | sgd         |            0.2  |         1500 |                  0    |     173 |   0.592377 |    0.67356  | 0.528658 |            0.67    |        0.33    |
| relu         |           64 | [64,32]         | sgd         |            0.2  |         1500 |                  0    |     358 |   0.626584 |    0.828947 | 0.503636 |            0.65875 |        0.34125 |
| relu         |           64 | [128,64]        | sgd         |            0.2  |         1500 |                  0    |     726 |   0.554182 |    0.614844 | 0.504416 |            0.6575  |        0.3425  |
| relu         |           64 | [128,64]        | bgd         |            0.2  |         1500 |                  0    |     173 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,64,32]      | bgd         |            0.1  |         1500 |                  0    |     277 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,64,32]      | bgd         |            0.2  |         1500 |                  0    |     457 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,64,32]      | mbgd        |            0.2  |         1500 |                  0    |     170 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,64,32]      | mbgd        |            0.2  |         1500 |                  0    |     150 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,32]         | bgd         |            0.2  |         1500 |                  0    |     154 |   0.554256 |    0.603704 | 0.512294 |            0.65875 |        0.34125 |
| relu         |           64 | [64,64,32]      | mbgd        |            0.2  |         1500 |                  0    |     279 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,64,32]      | mbgd        |            0.2  |         1500 |                  0    |     294 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,64,32]      | mbgd        |            0.2  |         1500 |                  0    |     156 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,64,32]      | bgd         |            0.2  |         1500 |                  0    |     170 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           32 | [64,64,32]      | mbgd        |            0.1  |         1500 |                  0    |     382 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,64,32]      | bgd         |            0.2  |         1500 |                  0    |     218 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,64,32]      | mbgd        |            0.2  |         1500 |                  0    |     161 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,64,32]      | mbgd        |            0.2  |         1500 |                  0    |     716 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [128,64]        | mbgd        |            0.2  |         1500 |                  0    |     626 |   0.497277 |    0.494752 | 0.499827 |            0.65375 |        0.34625 |
| relu         |           64 | [64,32]         | bgd         |            0.2  |         1500 |                  0    |     259 |   0.591243 |    0.705177 | 0.509004 |            0.66125 |        0.33875 |
| relu         |           64 | [128,64]        | bgd         |            0.2  |         1500 |                  0    |     277 |   0.506855 |    0.511905 | 0.501905 |            0.64625 |        0.35375 |
| relu         |           64 | [64,64,32]      | mbgd        |            0.2  |         1500 |                  0    |     438 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,64,32]      | mbgd        |            0.2  |         1500 |                  0    |     136 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,64,32]      | bgd         |            0.2  |         1500 |                  0    |     169 |   0.47481  |    0.452889 | 0.498961 |            0.65375 |        0.34625 |
| relu         |           64 | [64,64,32]      | mbgd        |            0.2  |         1500 |                  0    |     155 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,64,32]      | mbgd        |            0.2  |         1500 |                  0    |     143 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           32 | [64,64,32]      | mbgd        |            0.1  |         1500 |                  0    |     258 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,32]         | sgd         |            0.2  |         1500 |                  0    |     311 |   0.553968 |    0.599463 | 0.514892 |            0.65875 |        0.34125 |
| relu         |           64 | [128,64]        | sgd         |            0.2  |         1500 |                  0    |     123 |   0.501177 |    0.502098 | 0.50026  |            0.6475  |        0.3525  |
| relu         |           64 | [64,64,32]      | mbgd        |            0.1  |         1500 |                  0    |     269 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,64,32]      | bgd         |            0.2  |         1500 |                  0    |     719 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [128,64]        | bgd         |            0.2  |         1500 |                  0    |     126 |   0.483723 |    0.470465 | 0.497749 |            0.64875 |        0.35125 |
| relu         |           64 | [64,64,32]      | bgd         |            0.2  |         1500 |                  0    |     689 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,64,32]      | mbgd        |            0.1  |         1500 |                  0    |     254 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,64,32]      | bgd         |            0.2  |         1500 |                  0    |     210 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| tanh         |           16 | [64,64,32]      | bgd         |            0.05 |         1500 |                  0    |     970 |   0.532479 |    0.532836 | 0.532121 |            0.5825  |        0.4175  |
| relu         |           64 | [64,64,32]      | mbgd        |            0.2  |         1500 |                  0    |     167 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [128,64]        | sgd         |            0.2  |         1500 |                  0    |     241 |   0.488093 |    0.477848 | 0.498788 |            0.65125 |        0.34875 |
| relu         |           64 | [64,64,32]      | mbgd        |            0.2  |         1500 |                  0    |     163 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,64,32]      | mbgd        |            0.05 |         1500 |                  0    |     391 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [128,64]        | bgd         |            0.2  |         1500 |                  0    |     166 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [128,64]        | sgd         |            0.2  |         1500 |                  0    |     337 |   0.497277 |    0.494752 | 0.499827 |            0.65375 |        0.34625 |
| relu         |           64 | [64,64,32]      | mbgd        |            0.2  |         1500 |                  0    |     163 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,64,32]      | bgd         |            0.2  |         1500 |                  0    |     152 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [128,64]        | mbgd        |            0.2  |         1500 |                  0    |     122 |   0.542629 |    0.580334 | 0.509524 |            0.65625 |        0.34375 |
| relu         |           64 | [64,64,32]      | mbgd        |            0.1  |         1500 |                  0    |     245 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,64,32]      | bgd         |            0.2  |         1500 |                  0    |     473 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |
| relu         |           64 | [64,64,32]      | mbgd        |            0.1  |         1500 |                  0    |     283 |   0.396226 |    0.328125 | 0.5      |            0.65625 |        0.34375 |

</details>

3. To assess the performance of the best model identified through hyperparameter tuning, evaluate it on the test set and report the metrics as outlined above

| Parameter                   | Value        |
|-----------------------------|--------------|
| Activation                  | sigmoid      |
| Batch Size                  | 16           |
| Hidden Layers               | [128, 64]    |
| Learning Rate               | 0.1          |
| Max Epochs                  | 1500         |
| Optimizer                   | sgd          |

On test set,

| Metric           | Value                          |
|------------------|--------------------------------|
| Accuracy         | 0.07                           |
| Precision        | 0.689729222513694              |
| Recall           | 0.6378038179033538             |
| F1 Score         | 0.6627510096480106             |
| Hamming Loss     | 0.2925                         |
| Hamming Accuracy | 0.7075                         |

## 2.7 Analysis

### Single lbel classifiction

True:
[2 3 3 3 3 3 4 3 5 1 3 3 2 3 2 2 3 3 3 3 2 2 2 3 3 3 3 3 3 4 3 2 3 3 2 2 3
 2 3 3 1 2 3 1 3 2 2 3 4 2 3 4 2 2 4 4 4 3 3 2 2 3 2 3 5 2 2 3 3 5 2 2 3 2
 3 2 2 2 3 3 2 4 3 2 3 2 2 3 3 2 4 2 2 3 3 2 2 2 0 2 3 2 0 3 2 3 3 2 3 3 2
 1 4 4 3] 

Predicted:
[2 3 4 2 3 2 3 3 3 2 3 3 2 3 2 2 3 3 3 3 2 2 2 2 3 3 2 2 3 3 3 2 3 3 3 3 3
 2 3 3 2 2 2 3 3 2 2 3 3 3 3 4 3 3 3 3 4 3 2 2 3 2 2 3 3 2 3 3 2 4 2 2 3 2
 2 2 2 2 3 3 2 3 3 2 3 2 2 3 2 2 3 2 3 2 3 2 2 2 2 2 3 2 2 3 2 3 3 2 3 2 2
 2 3 4 2]

## Class Distribution (Original → Adjusted)
0 → 3: Least frequent
5 → 8: Second least frequent
1 → 4: Third least frequent
4 → 7: Fourth most frequent
3 → 6: Second most frequent
2 → 5: Most frequent

## Class-wise Performance

### Class 3 
- Actual occurrences: 2
- Correct predictions: 0
- Accuracy: 0%

### Class 8 
- Actual occurrences: 3
- Correct predictions: 0
- Accuracy: 0%

### Class 4 
- Actual occurrences: 4
- Correct predictions: 0
- Accuracy: 0%

### Class 7 
- Actual occurrences: 8
- Correct predictions: 2
- Accuracy: 25%

### Class 6 
- Actual occurrences: 53
- Correct predictions: 37
- Accuracy: 69.81%

### Class 5
- Actual occurrences: 44
- Correct predictions: 30
- Accuracy: 68.18%

## Analysis

1. Best performing classes:
   - Class 6 (69.81% accuracy)
   - Class 5 (68.18% accuracy)
   
   These classes are also the two most frequent in the dataset, which  contributed to their better performance. The model had more examples to learn from, leading to more accurate predictions.

2. Moderate performing class:
   - Class 7 (25% accuracy)
   
   This class had fewer examples, which may have limited the model's ability to learn its characteristics effectively.

3. Worst performing classes:
   - Classes 3, 8, and 4 (0% accuracy)
   
   These classes had very few examples in the dataset, which severely limited the model's ability to learn and predict them accurately. The model likely defaulted to predicting the more common classes instead.

4. Overall trends:
   - The model's performance strongly correlates with the frequency of classes in the dataset.
   - There's a clear imbalance in the dataset, with classes 5 and 6 dominating.
   - The model struggles with rare classes, failing to predict them at all.


### Performance on Multi-Label Classification:
In the multi-label scenario, where the target is a multi-hot encoded vector of length 8, there are a few patterns to note:
1. **Class Confusion and Overlap:**
   - If there are overlapping labels in the multi-label output (i.e., one data point belongs to multiple classes), the model might find it difficult to distinguish between those classes, leading to higher classification errors.
   
2. **Rare Labels:**
   - Labels that rarely appear together may not be learned well by the model. If there are labels that occur in only a few instances, the MLP may not get enough data during training to learn how to associate those labels correctly.

### Potential Reasons for Class Performance:
1. **Class Imbalance:**
   - If the dataset has an imbalanced distribution of classes (as you hinted with Classes 4 and 5 being rare), the model is likely biased toward predicting the majority class. This could explain why your MLP frequently predicts Class 2 or Class 3 and performs poorly on Classes 0, 1, 4, and 5.

2. **Complexity of Features:**
   - If certain classes have more complex or overlapping features with other classes, the MLP might have a harder time distinguishing between them, especially if it's not deep enough to capture intricate patterns in the data.

3. **Training Data Limitations:**
   - If some classes are underrepresented in the dataset or have less distinguishing features compared to others, the model will naturally perform poorly on those classes. You might need to look at adding more training data for the underperforming classes or using techniques like SMOTE to generate synthetic examples.

### Next Steps:
- **Confusion Matrix:** It would help to create a confusion matrix to better visualize where the model is making classification errors. This would give you a clear picture of which specific classes are getting confused with each other.
- **Class Balancing Techniques:** Consider oversampling the underrepresented classes or using a weighted loss function to give more importance to the minority classes during training.
- **Further Model Tuning:** Adjusting the architecture of your MLP (e.g., more layers or neurons) or trying different activation functions could improve the model's ability to capture more complex relationships between classes.

Would you like help generating a confusion matrix or implementing any of these techniques?

### Multi-label Classification Analysis

Predictions [['beauty', 'books', 'home'], ['beauty'], ['clothing'], ['clothing', 'food'], ['sports'], ['electronics'], ['clothing', 'food'], ['books', 'electronics', 'sports'], ['beauty', 'clothing'], ['books', 'electronics'], ['books', 'home'], ['beauty', 'books', 'food'], ['furniture'], ['books', 'furniture', 'home'], ['home'], ['beauty'], ['food', 'furniture'], ['food', 'sports'], ['food'], ['books', 'home'], ['clothing'], ['clothing', 'furniture', 'home', 'sports'], ['beauty'], ['electronics'], ['books', 'clothing'], ['furniture'], ['books'], ['electronics', 'home'], ['electronics', 'furniture'], ['sports'], ['books'], ['sports'], ['beauty', 'furniture', 'sports'], ['furniture'], ['beauty'], ['food', 'home'], ['beauty'], ['sports'], ['books', 'home'], ['food', 'furniture', 'home'], ['clothing', 'sports'], ['sports'], ['home'], ['furniture', 'sports'], ['food', 'home'], ['beauty', 'books', 'clothing', 'sports'], ['beauty'], ['beauty'], ['beauty', 'food'], ['clothing'], ['books', 'electronics', 'sports'], ['beauty', 'food'], ['electronics'], ['beauty', 'clothing'], ['beauty', 'clothing', 'food'], ['electronics'], ['food', 'sports'], ['electronics'], ['books', 'home'], ['electronics', 'food', 'home'], ['home', 'sports'], ['beauty'], ['clothing', 'sports'], ['clothing', 'food'], ['beauty'], ['furniture'], ['clothing'], ['clothing'], ['books', 'food', 'furniture'], ['beauty', 'books'], ['books', 'food', 'home'], ['home'], ['clothing', 'electronics'], ['beauty', 'food'], ['clothing'], ['beauty', 'clothing'], ['food', 'furniture'], ['electronics', 'home'], ['beauty'], ['beauty'], ['sports'], ['sports'], ['sports'], ['clothing', 'food'], ['beauty', 'electronics'], ['books', 'home'], ['sports'], ['beauty', 'books'], ['furniture'], ['clothing', 'electronics', 'sports'], ['books', 'home'], ['food', 'furniture'], ['books', 'electronics'], ['beauty'], ['electronics'], ['food'], ['furniture'], ['books', 'electronics'], ['books', 'food'], ['books', 'electronics']]


Actual [['beauty', 'books', 'home'], ['beauty'], ['clothing'], ['clothing', 'food'], ['sports'], ['electronics'], ['clothing', 'food'], ['books', 'electronics', 'sports'], ['beauty', 'clothing'], ['books', 'electronics'], ['books', 'home'], ['beauty', 'books', 'food'], ['furniture'], ['books', 'furniture', 'home'], ['home'], ['beauty'], ['food', 'furniture'], ['food', 'sports'], ['food'], ['books', 'home'], ['clothing'], ['clothing', 'furniture', 'home', 'sports'], ['beauty'], ['electronics'], ['books', 'clothing'], ['furniture'], ['books'], ['electronics', 'home'], ['electronics', 'furniture'], ['sports'], ['books'], ['sports'], ['beauty', 'furniture', 'sports'], ['furniture'], ['beauty'], ['food', 'home'], ['beauty'], ['sports'], ['books', 'home'], ['food', 'furniture', 'home'], ['clothing', 'sports'], ['sports'], ['home'], ['furniture', 'sports'], ['food', 'home'], ['beauty', 'books', 'clothing', 'sports'], ['beauty'], ['beauty'], ['beauty', 'food'], ['clothing'], ['books', 'electronics', 'sports'], ['beauty', 'food'], ['electronics'], ['beauty', 'clothing'], ['beauty', 'clothing', 'food'], ['electronics'], ['food', 'sports'], ['electronics'], ['books', 'home'], ['electronics', 'food', 'home'], ['home', 'sports'], ['beauty'], ['clothing', 'sports'], ['clothing', 'food'], ['beauty'], ['furniture'], ['clothing'], ['clothing'], ['books', 'food', 'furniture'], ['beauty', 'books'], ['books', 'food', 'home'], ['home'], ['clothing', 'electronics'], ['beauty', 'food'], ['clothing'], ['beauty', 'clothing'], ['food', 'furniture'], ['electronics', 'home'], ['beauty'], ['beauty'], ['sports'], ['sports'], ['sports'], ['clothing', 'food'], ['beauty', 'electronics'], ['books', 'home'], ['sports'], ['beauty', 'books'], ['furniture'], ['clothing', 'electronics', 'sports'], ['books', 'home'], ['food', 'furniture'], ['books', 'electronics'], ['beauty'], ['electronics'], ['food'], ['furniture'], ['books', 'electronics'], ['books', 'food'], ['books', 'electronics'], ['beauty', 'food', 'home', 'sports'], ['beauty', 'furniture', 'home', 'sports'], ['clothing', 'food', 'furniture', 'home'], ['beauty', 'clothing', 'furniture'], ['sports'], ['electronics', 'food'], ['food', 'furniture'], ['beauty', 'books', 'home', 'sports'], ['beauty', 'books', 'furniture'], ['beauty', 'clothing', 'electronics', 'food', 'home', 'sports'], ['beauty', 'books', 'clothing', 'sports'], ['beauty', 'sports'], ['clothing', 'electronics', 'furniture', 'sports'], ['home'], ['home'], ['beauty', 'electronics', 'furniture'], ['clothing', 'electronics', 'food', 'furniture', 'sports'], ['food'], ['beauty', 'clothing', 'electronics', 'food'], ['beauty', 'food', 'home', 'sports'], ['clothing'], ['books', 'home'], ['beauty', 'electronics'], ['books', 'electronics'], ['books', 'clothing', 'furniture'], ['beauty', 'electronics', 'furniture'], ['books', 'clothing', 'electronics'], ['food', 'home'], ['clothing', 'electronics', 'home'], ['furniture', 'sports'], ['books', 'clothing'], ['sports'], ['clothing', 'sports'], ['beauty', 'furniture', 'sports'], ['beauty', 'furniture'], ['books', 'home', 'sports'], ['beauty', 'food', 'sports'], ['sports'], ['beauty', 'books'], ['electronics', 'food', 'home'], ['clothing', 'food', 'furniture'], ['clothing', 'furniture', 'home', 'sports'], ['books', 'clothing', 'electronics', 'home'], ['clothing', 'electronics', 'home', 'sports'], ['clothing', 'home', 'sports'], ['clothing', 'electronics', 'furniture', 'sports'], ['beauty'], ['beauty', 'books'], ['beauty', 'home'], ['clothing', 'food'], ['books'], ['beauty', 'books', 'clothing', 'furniture', 'home'], ['clothing', 'electronics', 'furniture', 'home'], ['clothing', 'electronics', 'home'], ['food', 'home'], ['beauty', 'clothing', 'electronics', 'home'], ['food'], ['beauty', 'electronics'], ['clothing', 'food', 'home'], ['beauty', 'food', 'home'], ['beauty', 'books', 'clothing', 'food', 'sports'], ['beauty', 'clothing', 'home'], ['beauty', 'books', 'sports'], ['clothing'], ['beauty', 'home', 'sports'], ['beauty', 'books', 'electronics', 'furniture', 'home'], ['beauty', 'clothing', 'food'], ['books', 'clothing', 'food', 'furniture', 'home'], ['books', 'electronics', 'food', 'sports'], ['beauty', 'furniture', 'sports'], ['beauty', 'food'], ['beauty', 'home', 'sports'], ['books', 'clothing', 'sports'], ['electronics', 'food', 'home', 'sports'], ['beauty', 'clothing', 'electronics'], ['clothing', 'furniture'], ['beauty', 'books', 'furniture', 'sports'], ['clothing', 'electronics', 'furniture', 'home'], ['beauty', 'food', 'furniture', 'home'], ['beauty', 'clothing'], ['books', 'electronics', 'food', 'home', 'sports'], ['books', 'sports'], ['electronics', 'home', 'sports'], ['food', 'furniture'], ['beauty'], ['books', 'clothing', 'electronics', 'food'], ['electronics', 'sports'], ['beauty'], ['beauty', 'books', 'furniture'], ['clothing'], ['books', 'furniture', 'sports'], ['books', 'electronics', 'furniture', 'sports'], ['books', 'electronics', 'home'], ['beauty', 'clothing', 'electronics', 'food'], ['electronics', 'food', 'sports'], ['books', 'food'], ['beauty', 'clothing', 'food', 'furniture'], ['books', 'electronics'], ['beauty', 'books', 'clothing', 'food', 'home'], ['beauty', 'books', 'clothing', 'furniture']]

1. Beauty
   - Generally good performance
   - Often correctly identified
   - Some false negatives (missed classifications)

2. Books
   - Moderate performance
   - Some accurate predictions, but also misses

3. Clothing
   - Mixed performance
   - Some accurate predictions, but also misses and false positives

4. Electronics
   - Good performance
   - Often correctly identified when present
   - Few false positives

5. Food
   - Moderate performance
   - Some accurate predictions, but also misses

6. Furniture
   - Poor performance
   - Often missed or incorrectly predicted

7. Home
   - Moderate performance
   - Some accurate predictions, but also misses

8. Sports
   - Good performance
   - Often correctly identified when present
   - Few false positives

1. Best performing classes:
   - Electronics and Sports
   - Reason: These categories might have more distinct features or patterns that the model can easily recognize.

2. Worst performing class:
   - Furniture
   - Reason: This category might have overlapping features with other categories (e.g., Home) or might not have enough distinct characteristics for the model to learn.

3. Moderate performing classes:
   - Books, Clothing, Food, and Home
   - These categories might have some distinct features but also share similarities with other categories, making classification more challenging.

4. Beauty category:
   - Good performance overall, but with some missed classifications
   - This could be due to its potential overlap with other categories like Clothing or Home

## Exact Matches

1. ['beauty']
2. ['clothing']
3. ['sports']
4. ['electronics']
5. ['furniture']
6. ['books']
7. ['home']
8. ['food']
9. ['books', 'electronics']
10. ['clothing', 'food']

These exact matches demonstrate that the model correctly predicts single-label instances much more often, and 2 label instances at maximum. Beyond 3 labels, the predictions never match exactly.

- Accuracy: 0.07 (This seems low, but it's for exact matches across all labels, which is challenging in multi-label classification)
- Precision: 0.6897 (About 69% of predicted labels are correct)
- Recall: 0.6378 (About 64% of actual labels are correctly predicted)
- F1 Score: 0.6628 
- Hamming Accuracy: 0.7075 (About 71% of individual label predictions are correct)

These metrics suggest that while the model struggles with exact matches across all labels (low accuracy), it performs reasonably well in predicting individual labels (higher Hamming Accuracy, Precision, and Recall).






| Attribute   |       Mean |   Standard Deviation |       Min |      Max |
|:------------|-----------:|---------------------:|----------:|---------:|
| CRIM        |   3.55973  |             8.65049  |   0.00632 |  88.9762 |
| ZN          |  11.0425   |            23.2138   |   0       | 100      |
| INDUS       |  11.856    |            18.3705   |   0.46    | 391      |
| CHAS        |   0.107692 |             0.896361 |   0       |  19.2    |
| NOX         |   1.32609  |            17.4654   |   0.385   | 396.9    |
| RM          |   6.29777  |             0.757256 |   3.561   |  12.92   |
| AGE         |  68.5609   |            27.895    |   2.9     | 100      |
| DIS         |   3.77586  |             2.09562  |   1.1296  |  12.1265 |
| RAD         |   9.45224  |             8.68252  |   1       |  24      |
| TAX         | 406.852    |           167.748    | 187       | 711      |
| PTRATIO     |  18.4832   |             2.16296  |  12.6     |  22      |
| B           | 357.204    |            90.6927   |   0.32    | 396.9    |
| LSTAT       |  12.6793   |             7.12381  |   1.73    |  37.97   |
| MEDV        |  22.4887   |             9.14391  |   5       |  50      |

![Histograms](./figures/regression_distributions.png)

Epoch 1/10, Loss: 6.259334192820956
Epoch 2/10, Loss: 5.548498327557013
Epoch 3/10, Loss: 4.964977160225369
Epoch 4/10, Loss: 4.483288198096867
Epoch 5/10, Loss: 4.082735406606526
Epoch 6/10, Loss: 3.7468096620742517
Epoch 7/10, Loss: 3.462558163379566
Epoch 8/10, Loss: 3.219918084870864
Epoch 9/10, Loss: 3.011087371211398
Epoch 10/10, Loss: 2.829989667505953

Classification Task Scores Without Auto Encoders

Accuracy: 0.2561
  Precision (macro): 0.2340
  Recall (macro): 0.2472
  F1-Score (macro): 0.2405
  Precision (micro): 0.2561
  Recall (micro): 0.2561
  F1-Score (micro): 0.2561
time_taken1=55.996248960494995

Classification Task Scores

Accuracy: 0.1402
  Precision (macro): 0.1161
  Recall (macro): 0.1356
  F1-Score (macro): 0.1251
  Precision (micro): 0.1402
  Recall (micro): 0.1402
  F1-Score (micro): 0.1402
time_taken2=46.165589809417725

Note, similar accuracy using 10 epochs, 0.01 vs 200 epochs, 0.2 grad descent, which is weird, best with sigmoid

Epoch 0, Train Loss: 0.0576, Train MSE: 0.0576, Validation MSE: 0.0649
Epoch 10, Train Loss: 0.0688, Train MSE: 0.0688, Validation MSE: 0.0768
Epoch 20, Train Loss: 0.0687, Train MSE: 0.0687, Validation MSE: 0.0774
Epoch 30, Train Loss: 0.0686, Train MSE: 0.0686, Validation MSE: 0.0777
Epoch 40, Train Loss: 0.0686, Train MSE: 0.0686, Validation MSE: 0.0781
Epoch 50, Train Loss: 0.0687, Train MSE: 0.0687, Validation MSE: 0.0786
Early stopping at epoch 52
MSE: 0.01242521717995083            
MAE: 0.07777702730961404            
RMSE: 0.1114684582290023            
R2 Score: 0.7153900598629171
Epoch 1/10, Loss: 3.4216108860291494
Epoch 2/10, Loss: 2.612487385441516
Epoch 3/10, Loss: 2.177920321281458
Epoch 4/10, Loss: 1.9097271345974087
Epoch 5/10, Loss: 1.729737250342207
Epoch 6/10, Loss: 1.601862523298113
Epoch 7/10, Loss: 1.506940171627335
Epoch 8/10, Loss: 1.4344477427370441
Epoch 9/10, Loss: 1.3781229710056127
Epoch 10/10, Loss: 1.3338152897677653
Classification Task Scores
---------------------------
Accuracy: 0.2561
  Precision (macro): 0.2340
  Recall (macro): 0.2472
  F1-Score (macro): 0.2405
  Precision (micro): 0.2561
  Recall (micro): 0.2561
  F1-Score (micro): 0.2561
---------------------------
time_taken1=71.2023024559021
Classification Task Scores
---------------------------
Accuracy: 0.1168
  Precision (macro): 0.0929
  Recall (macro): 0.1132
  F1-Score (macro): 0.1021
  Precision (micro): 0.1168
  Recall (micro): 0.1168
  F1-Score (micro): 0.1168
---------------------------
time_taken2=60.10706067085266


Epoch 0, Train Loss: 4.6611, Train Accuracy: 0.0284, Validation Accuracy: 0.0284
Epoch 1, Train Loss: 4.6142, Train Accuracy: 0.0422, Validation Accuracy: 0.0424
Epoch 2, Train Loss: 4.5588, Train Accuracy: 0.0494, Validation Accuracy: 0.0504
Epoch 3, Train Loss: 4.5001, Train Accuracy: 0.0590, Validation Accuracy: 0.0590
Epoch 4, Train Loss: 4.4364, Train Accuracy: 0.0711, Validation Accuracy: 0.0732
Epoch 5, Train Loss: 4.3591, Train Accuracy: 0.0795, Validation Accuracy: 0.0814
Epoch 6, Train Loss: 4.2994, Train Accuracy: 0.0875, Validation Accuracy: 0.0856
Epoch 7, Train Loss: 4.2509, Train Accuracy: 0.0937, Validation Accuracy: 0.0937
Epoch 8, Train Loss: 4.2035, Train Accuracy: 0.1015, Validation Accuracy: 0.1011
Epoch 9, Train Loss: 4.1601, Train Accuracy: 0.1106, Validation Accuracy: 0.1135
Epoch 10, Train Loss: 4.1255, Train Accuracy: 0.1159, Validation Accuracy: 0.1152
Epoch 11, Train Loss: 4.0851, Train Accuracy: 0.1165, Validation Accuracy: 0.1183
Epoch 12, Train Loss: 4.0427, Train Accuracy: 0.1218, Validation Accuracy: 0.1204
Epoch 13, Train Loss: 4.0031, Train Accuracy: 0.1270, Validation Accuracy: 0.1296
Epoch 14, Train Loss: 3.9584, Train Accuracy: 0.1293, Validation Accuracy: 0.1297
Epoch 15, Train Loss: 3.9193, Train Accuracy: 0.1324, Validation Accuracy: 0.1341
Epoch 16, Train Loss: 3.8784, Train Accuracy: 0.1407, Validation Accuracy: 0.1422
Epoch 17, Train Loss: 3.8436, Train Accuracy: 0.1413, Validation Accuracy: 0.1422
Epoch 18, Train Loss: 3.7834, Train Accuracy: 0.1454, Validation Accuracy: 0.1503
Epoch 19, Train Loss: 3.7368, Train Accuracy: 0.1538, Validation Accuracy: 0.1551
Epoch 20, Train Loss: 3.6829, Train Accuracy: 0.1619, Validation Accuracy: 0.1636
Epoch 21, Train Loss: 3.6554, Train Accuracy: 0.1612, Validation Accuracy: 0.1643
Epoch 22, Train Loss: 3.5942, Train Accuracy: 0.1720, Validation Accuracy: 0.1739
Epoch 23, Train Loss: 3.5529, Train Accuracy: 0.1781, Validation Accuracy: 0.1795
Epoch 24, Train Loss: 3.5299, Train Accuracy: 0.1766, Validation Accuracy: 0.1801
Epoch 25, Train Loss: 3.4887, Train Accuracy: 0.1852, Validation Accuracy: 0.1907
Epoch 26, Train Loss: 3.4565, Train Accuracy: 0.1883, Validation Accuracy: 0.1885
Epoch 27, Train Loss: 3.4267, Train Accuracy: 0.1900, Validation Accuracy: 0.1916
Epoch 28, Train Loss: 3.4136, Train Accuracy: 0.1893, Validation Accuracy: 0.1918
Epoch 29, Train Loss: 3.3789, Train Accuracy: 0.1957, Validation Accuracy: 0.1959
Epoch 30, Train Loss: 3.3574, Train Accuracy: 0.1994, Validation Accuracy: 0.1977
Epoch 31, Train Loss: 3.3391, Train Accuracy: 0.2003, Validation Accuracy: 0.2032
Epoch 32, Train Loss: 3.3114, Train Accuracy: 0.2068, Validation Accuracy: 0.2082
Epoch 33, Train Loss: 3.2833, Train Accuracy: 0.2103, Validation Accuracy: 0.2118
Epoch 34, Train Loss: 3.2731, Train Accuracy: 0.2130, Validation Accuracy: 0.2091
Epoch 35, Train Loss: 3.2768, Train Accuracy: 0.2079, Validation Accuracy: 0.2075
Epoch 36, Train Loss: 3.2289, Train Accuracy: 0.2147, Validation Accuracy: 0.2162
Epoch 37, Train Loss: 3.2190, Train Accuracy: 0.2188, Validation Accuracy: 0.2162
Epoch 38, Train Loss: 3.1923, Train Accuracy: 0.2239, Validation Accuracy: 0.2249
Epoch 39, Train Loss: 3.1920, Train Accuracy: 0.2224, Validation Accuracy: 0.2205
Epoch 40, Train Loss: 3.1569, Train Accuracy: 0.2281, Validation Accuracy: 0.2300
Epoch 41, Train Loss: 3.1542, Train Accuracy: 0.2275, Validation Accuracy: 0.2268
Epoch 42, Train Loss: 3.1404, Train Accuracy: 0.2287, Validation Accuracy: 0.2292
Epoch 43, Train Loss: 3.1424, Train Accuracy: 0.2283, Validation Accuracy: 0.2278
Epoch 44, Train Loss: 3.1213, Train Accuracy: 0.2327, Validation Accuracy: 0.2331
Epoch 45, Train Loss: 3.0988, Train Accuracy: 0.2366, Validation Accuracy: 0.2358
Epoch 46, Train Loss: 3.1008, Train Accuracy: 0.2354, Validation Accuracy: 0.2324
Epoch 47, Train Loss: 3.0877, Train Accuracy: 0.2372, Validation Accuracy: 0.2344
Epoch 48, Train Loss: 3.0772, Train Accuracy: 0.2391, Validation Accuracy: 0.2381
Epoch 49, Train Loss: 3.0718, Train Accuracy: 0.2387, Validation Accuracy: 0.2397
Epoch 50, Train Loss: 3.0766, Train Accuracy: 0.2376, Validation Accuracy: 0.2383
Epoch 51, Train Loss: 3.0447, Train Accuracy: 0.2444, Validation Accuracy: 0.2442
Epoch 52, Train Loss: 3.0656, Train Accuracy: 0.2389, Validation Accuracy: 0.2370
Epoch 53, Train Loss: 3.0437, Train Accuracy: 0.2433, Validation Accuracy: 0.2442
Epoch 54, Train Loss: 3.0401, Train Accuracy: 0.2426, Validation Accuracy: 0.2438
Epoch 55, Train Loss: 3.0299, Train Accuracy: 0.2462, Validation Accuracy: 0.2486
Epoch 56, Train Loss: 3.0315, Train Accuracy: 0.2419, Validation Accuracy: 0.2410
Epoch 57, Train Loss: 3.0197, Train Accuracy: 0.2477, Validation Accuracy: 0.2449
Epoch 58, Train Loss: 3.0271, Train Accuracy: 0.2428, Validation Accuracy: 0.2462
Epoch 59, Train Loss: 3.0135, Train Accuracy: 0.2458, Validation Accuracy: 0.2467
Epoch 60, Train Loss: 3.0056, Train Accuracy: 0.2504, Validation Accuracy: 0.2515
Epoch 61, Train Loss: 2.9940, Train Accuracy: 0.2514, Validation Accuracy: 0.2528
Epoch 62, Train Loss: 3.0000, Train Accuracy: 0.2477, Validation Accuracy: 0.2476
Epoch 63, Train Loss: 2.9880, Train Accuracy: 0.2523, Validation Accuracy: 0.2499
Epoch 64, Train Loss: 2.9934, Train Accuracy: 0.2534, Validation Accuracy: 0.2488
Epoch 65, Train Loss: 2.9800, Train Accuracy: 0.2535, Validation Accuracy: 0.2529
Epoch 66, Train Loss: 2.9910, Train Accuracy: 0.2494, Validation Accuracy: 0.2508
Epoch 67, Train Loss: 2.9746, Train Accuracy: 0.2524, Validation Accuracy: 0.2528
Epoch 68, Train Loss: 2.9702, Train Accuracy: 0.2553, Validation Accuracy: 0.2549
Epoch 69, Train Loss: 2.9774, Train Accuracy: 0.2542, Validation Accuracy: 0.2489
Epoch 70, Train Loss: 2.9597, Train Accuracy: 0.2562, Validation Accuracy: 0.2548
Epoch 71, Train Loss: 2.9568, Train Accuracy: 0.2571, Validation Accuracy: 0.2556
Epoch 72, Train Loss: 2.9551, Train Accuracy: 0.2582, Validation Accuracy: 0.2593
Epoch 73, Train Loss: 2.9694, Train Accuracy: 0.2548, Validation Accuracy: 0.2548
Epoch 74, Train Loss: 2.9428, Train Accuracy: 0.2591, Validation Accuracy: 0.2562
Epoch 75, Train Loss: 2.9452, Train Accuracy: 0.2593, Validation Accuracy: 0.2611
Epoch 76, Train Loss: 2.9519, Train Accuracy: 0.2562, Validation Accuracy: 0.2544
Epoch 77, Train Loss: 2.9298, Train Accuracy: 0.2610, Validation Accuracy: 0.2609
Epoch 78, Train Loss: 2.9383, Train Accuracy: 0.2593, Validation Accuracy: 0.2581
Epoch 79, Train Loss: 2.9269, Train Accuracy: 0.2607, Validation Accuracy: 0.2604
Epoch 80, Train Loss: 2.9399, Train Accuracy: 0.2549, Validation Accuracy: 0.2561
Epoch 81, Train Loss: 2.9338, Train Accuracy: 0.2607, Validation Accuracy: 0.2584
Epoch 82, Train Loss: 2.9193, Train Accuracy: 0.2607, Validation Accuracy: 0.2618
Epoch 83, Train Loss: 2.9185, Train Accuracy: 0.2629, Validation Accuracy: 0.2616
Epoch 84, Train Loss: 2.9199, Train Accuracy: 0.2625, Validation Accuracy: 0.2594
Epoch 85, Train Loss: 2.9077, Train Accuracy: 0.2631, Validation Accuracy: 0.2582
Epoch 86, Train Loss: 2.9404, Train Accuracy: 0.2554, Validation Accuracy: 0.2519
Epoch 87, Train Loss: 2.9135, Train Accuracy: 0.2609, Validation Accuracy: 0.2574
Epoch 88, Train Loss: 2.9087, Train Accuracy: 0.2638, Validation Accuracy: 0.2616
Epoch 89, Train Loss: 2.8975, Train Accuracy: 0.2647, Validation Accuracy: 0.2615
Epoch 90, Train Loss: 2.9002, Train Accuracy: 0.2631, Validation Accuracy: 0.2628
Epoch 91, Train Loss: 2.9031, Train Accuracy: 0.2643, Validation Accuracy: 0.2607
Epoch 92, Train Loss: 2.9288, Train Accuracy: 0.2599, Validation Accuracy: 0.2591
Epoch 93, Train Loss: 2.8896, Train Accuracy: 0.2672, Validation Accuracy: 0.2592
Epoch 94, Train Loss: 2.8895, Train Accuracy: 0.2656, Validation Accuracy: 0.2624
Epoch 95, Train Loss: 2.8793, Train Accuracy: 0.2688, Validation Accuracy: 0.2674
Epoch 96, Train Loss: 2.8795, Train Accuracy: 0.2668, Validation Accuracy: 0.2664
Epoch 97, Train Loss: 2.9060, Train Accuracy: 0.2641, Validation Accuracy: 0.2594
Epoch 98, Train Loss: 2.8859, Train Accuracy: 0.2651, Validation Accuracy: 0.2637
Epoch 99, Train Loss: 2.8848, Train Accuracy: 0.2667, Validation Accuracy: 0.2664
Test Accuracy: 0.25776281331836887
Precision: 0.237878828969582
Recall: 0.25033297455881737
F1 Score: 0.24394705089124158