{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..', '..', 'models', 'cnn')))\n",
    "from cnn import CNN\n",
    "from multilabel_cnn import MultiLabelCNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function load mnist data() that extracts images from the\n",
    "dataset folders and organizes them into separate lists for images and labels\n",
    "corresponding to the train, validation, and test splits. Ensure that the\n",
    "images are loaded from their respective folders without any overlap or\n",
    "mixing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_digit(digit):\n",
    "    \"\"\"Creates an 11-dimensional one-hot vector for a single digit or blank.\"\"\"\n",
    "    one_hot = np.zeros(11)\n",
    "    if digit == '*':\n",
    "        one_hot[10] = 1 \n",
    "    else:\n",
    "        one_hot[int(digit)] = 1\n",
    "    return one_hot\n",
    "\n",
    "def one_hot_encode_label(label):\n",
    "    \"\"\"Encodes the label (up to three digits) as a 33-dimensional vector.\"\"\"\n",
    "    label_str = str(label).ljust(3, '*')\n",
    "    one_hot_encoded = np.concatenate([one_hot_encode_digit(digit) for digit in label_str])\n",
    "    return one_hot_encoded\n",
    "\n",
    "\n",
    "def load_mnist_data(base_path='../../data/external/double_mnist', use_label_count=False):\n",
    "    images = {\n",
    "        'train': [],\n",
    "        'val': [],\n",
    "        'test': []\n",
    "    }\n",
    "    labels = {\n",
    "        'train': [],\n",
    "        'val': [],\n",
    "        'test': []\n",
    "    }\n",
    "\n",
    "    splits = ['train', 'val', 'test']\n",
    "    for split in splits:\n",
    "        split_path = os.path.join(base_path, split)\n",
    "        for label in os.listdir(split_path):\n",
    "            label_path = os.path.join(split_path, label)\n",
    "            if os.path.isdir(label_path):\n",
    "                for img_name in os.listdir(label_path):\n",
    "                    img_path = os.path.join(label_path, img_name)\n",
    "                    img = Image.open(img_path).convert('L')\n",
    "                    img_array = np.array(img)\n",
    "                    images[split].append(img_array)\n",
    "\n",
    "                    if use_label_count:\n",
    "                        if label == \"0\":\n",
    "                            label_count = 0\n",
    "                        else:\n",
    "                            label_count = len(label)\n",
    "                        labels[split].append(label_count)\n",
    "                    else:\n",
    "                        labels[split].append(one_hot_encode_label(label))\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "images, labels = load_mnist_data(use_label_count=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class called MultiMNISTDataset that will be used to create\n",
    "dataloaders for training and evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class MultiMNISTDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        img = torch.tensor(img, dtype=torch.float32) / 255.0  \n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),         \n",
    "    transforms.Resize((28, 28)),     \n",
    "    transforms.ToTensor(),         \n",
    "])\n",
    "\n",
    "train_dataset = MultiMNISTDataset(images['train'], labels['train'], transform=transform)\n",
    "val_dataset = MultiMNISTDataset(images['val'], labels['val'], transform=transform)\n",
    "test_dataset = MultiMNISTDataset(images['test'], labels['test'], transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the CNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./figures/cnn_loss_plots/plot.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/5:  32%|███▏      | 125/394 [00:04<00:08, 31.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m CNN(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_conv_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, optimizer_choice\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/semester5/smai/assignments/smai-m24-assignments-StarryBadger/models/cnn/cnn.py:63\u001b[0m, in \u001b[0;36mCNN.fit\u001b[0;34m(self, train_loader, val_loader, epochs, lr)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     62\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     64\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/semester5/smai/assignments/smai-m24-assignments-StarryBadger/venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/semester5/smai/assignments/smai-m24-assignments-StarryBadger/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/semester5/smai/assignments/smai-m24-assignments-StarryBadger/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/semester5/smai/assignments/smai-m24-assignments-StarryBadger/venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[4], line 21\u001b[0m, in \u001b[0;36mMultiMNISTDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(img, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 21\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img, label\n",
      "File \u001b[0;32m~/semester5/smai/assignments/smai-m24-assignments-StarryBadger/venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/semester5/smai/assignments/smai-m24-assignments-StarryBadger/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/semester5/smai/assignments/smai-m24-assignments-StarryBadger/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/semester5/smai/assignments/smai-m24-assignments-StarryBadger/venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/semester5/smai/assignments/smai-m24-assignments-StarryBadger/venv/lib/python3.12/site-packages/torchvision/transforms/functional.py:477\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    475\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[0;32m~/semester5/smai/assignments/smai-m24-assignments-StarryBadger/venv/lib/python3.12/site-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/semester5/smai/assignments/smai-m24-assignments-StarryBadger/venv/lib/python3.12/site-packages/PIL/Image.py:2328\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2316\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2317\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2318\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[1;32m   2319\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2320\u001b[0m         )\n\u001b[1;32m   2321\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2322\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2323\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2324\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2325\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2326\u001b[0m         )\n\u001b[0;32m-> 2328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNN(task='regression', num_classes=4, num_conv_layers=1,dropout_rate=0.5, optimizer_choice='adam', device=device).to(device)\n",
    "model.fit(train_loader, val_loader, epochs=5, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   True Labels  Predictions Correctness\n",
      "0            3          3.0           ✓\n",
      "1            3          3.0           ✓\n",
      "2            3          3.0           ✓\n",
      "3            3          2.0           x\n",
      "4            3          2.0           x\n",
      "Results saved to predictions.csv\n",
      "Test Accuracy: 0.7967\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def calculate_accuracy(model, test_loader, output_file='predictions.csv'):\n",
    "    model.eval()\n",
    "    predictions = model.predict(test_loader)\n",
    "    \n",
    "    true_labels = []\n",
    "    for _, labels in test_loader:\n",
    "        true_labels.append(labels)\n",
    "    true_labels = torch.cat(true_labels).cpu()\n",
    "    predictions = torch.round(predictions).cpu()\n",
    "    \n",
    "    # Create a DataFrame to display true labels, predictions, and correctness\n",
    "    results_df = pd.DataFrame({\n",
    "        'True Labels': true_labels.numpy(),\n",
    "        'Predictions': predictions.numpy(),\n",
    "        'Correctness': ['✓' if true == pred else 'x' for true, pred in zip(true_labels.numpy(), predictions.numpy())]\n",
    "    })\n",
    "    \n",
    "    print(results_df.head())\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "accuracy = calculate_accuracy(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./figures/cnn_loss_plots/classification_loss_combination_1_lr_0.001_dropout_0.3_layers_2_opt_adam.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/2: 100%|██████████| 394/394 [00:14<00:00, 27.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Training Loss: 0.2117, Validation Loss: 0.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/2: 100%|██████████| 394/394 [00:14<00:00, 26.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Training Loss: 0.0083, Validation Loss: 0.0032\n",
      "./figures/cnn_loss_plots/classification_loss_combination_2_lr_0.001_dropout_0.5_layers_3_opt_adam.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/2: 100%|██████████| 394/394 [00:16<00:00, 23.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Training Loss: 0.2935, Validation Loss: 0.0028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/2: 100%|██████████| 394/394 [00:12<00:00, 32.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Training Loss: 0.0105, Validation Loss: 0.0071\n",
      "./figures/cnn_loss_plots/classification_loss_combination_3_lr_0.0005_dropout_0.5_layers_2_opt_sgd.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/2: 100%|██████████| 394/394 [00:09<00:00, 41.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Training Loss: 1.1696, Validation Loss: 1.0771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/2: 100%|██████████| 394/394 [00:09<00:00, 41.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Training Loss: 1.1167, Validation Loss: 1.0460\n",
      "./figures/cnn_loss_plots/classification_loss_combination_4_lr_0.0005_dropout_0.3_layers_3_opt_sgd.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/2: 100%|██████████| 394/394 [00:11<00:00, 35.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Training Loss: 1.2357, Validation Loss: 1.0965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/2: 100%|██████████| 394/394 [00:11<00:00, 33.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Training Loss: 1.1357, Validation Loss: 1.0845\n",
      "./figures/cnn_loss_plots/classification_loss_combination_5_lr_0.0001_dropout_0.5_layers_4_opt_adam.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/2: 100%|██████████| 394/394 [00:13<00:00, 28.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Training Loss: 0.7668, Validation Loss: 0.0730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/2: 100%|██████████| 394/394 [00:13<00:00, 29.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Training Loss: 0.0397, Validation Loss: 0.0144\n",
      "./figures/cnn_loss_plots/regression_loss_combination_1_lr_0.001_dropout_0.3_layers_2_opt_adam.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/2: 100%|██████████| 394/394 [00:11<00:00, 34.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Training Loss: 0.1814, Validation Loss: 0.0304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/2: 100%|██████████| 394/394 [00:11<00:00, 34.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Training Loss: 0.0627, Validation Loss: 0.0196\n",
      "./figures/cnn_loss_plots/regression_loss_combination_2_lr_0.001_dropout_0.5_layers_3_opt_adam.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/2: 100%|██████████| 394/394 [00:13<00:00, 29.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Training Loss: 0.2237, Validation Loss: 0.0644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/2: 100%|██████████| 394/394 [00:12<00:00, 31.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Training Loss: 0.0965, Validation Loss: 0.1127\n",
      "./figures/cnn_loss_plots/regression_loss_combination_3_lr_0.0005_dropout_0.5_layers_2_opt_sgd.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/2: 100%|██████████| 394/394 [00:11<00:00, 34.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Training Loss: 0.6034, Validation Loss: 0.2043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/2: 100%|██████████| 394/394 [00:11<00:00, 35.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Training Loss: 0.2040, Validation Loss: 0.1193\n",
      "./figures/cnn_loss_plots/regression_loss_combination_4_lr_0.0005_dropout_0.3_layers_3_opt_sgd.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/2: 100%|██████████| 394/394 [00:12<00:00, 30.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Training Loss: 0.7498, Validation Loss: 0.2468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/2: 100%|██████████| 394/394 [00:12<00:00, 31.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Training Loss: 0.2516, Validation Loss: 0.1488\n",
      "./figures/cnn_loss_plots/regression_loss_combination_5_lr_0.0001_dropout_0.5_layers_4_opt_adam.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/2: 100%|██████████| 394/394 [00:12<00:00, 31.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Training Loss: 0.6441, Validation Loss: 0.1403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/2: 100%|██████████| 394/394 [00:12<00:00, 30.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Training Loss: 0.1625, Validation Loss: 0.0365\n",
      "Best Classification Model Combination: {'lr': 0.001, 'dropout_rate': 0.3, 'num_conv_layers': 2, 'optimizer_choice': 'adam'}\n",
      "Best Regression Model Combination: {'lr': 0.001, 'dropout_rate': 0.3, 'num_conv_layers': 2, 'optimizer_choice': 'adam'}\n",
      "Best parameters saved to ./figures/cnn_loss_plots/best_model_params.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "hyperparam_combinations = [\n",
    "    {'lr': 0.001, 'dropout_rate': 0.3, 'num_conv_layers': 2, 'optimizer_choice': 'adam'},\n",
    "    {'lr': 0.001, 'dropout_rate': 0.5, 'num_conv_layers': 3, 'optimizer_choice': 'adam'},\n",
    "    {'lr': 0.0005, 'dropout_rate': 0.5, 'num_conv_layers': 2, 'optimizer_choice': 'sgd'},\n",
    "    {'lr': 0.0005, 'dropout_rate': 0.3, 'num_conv_layers': 3, 'optimizer_choice': 'sgd'},\n",
    "    {'lr': 0.0001, 'dropout_rate': 0.5, 'num_conv_layers': 4, 'optimizer_choice': 'adam'}\n",
    "]\n",
    "\n",
    "plot_save_dir = \"./figures/cnn_loss_plots\"\n",
    "os.makedirs(plot_save_dir, exist_ok=True)\n",
    "\n",
    "best_classification_model, best_classification_loss, best_classification_combination = None, float('inf'), None\n",
    "best_regression_model, best_regression_loss, best_regression_combination = None, float('inf'), None\n",
    "\n",
    "for task in ['classification', 'regression']:\n",
    "    for idx, params in enumerate(hyperparam_combinations, 1):\n",
    "        loss_figure_save_path = os.path.join(\n",
    "            plot_save_dir,\n",
    "            f\"{task}_loss_combination_{idx}_lr_{params['lr']}_dropout_{params['dropout_rate']}_layers_{params['num_conv_layers']}_opt_{params['optimizer_choice']}.png\"\n",
    "        )\n",
    "        model = CNN(\n",
    "            task=task,\n",
    "            num_classes=4 if task == 'classification' else 1,\n",
    "            num_conv_layers=params['num_conv_layers'],\n",
    "            dropout_rate=params['dropout_rate'],\n",
    "            optimizer_choice=params['optimizer_choice'],\n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "            loss_figure_save_path = loss_figure_save_path\n",
    "        )\n",
    "\n",
    "        model.fit(train_loader, val_loader, epochs=2, lr=params['lr'])\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss() if task == 'classification' else nn.MSELoss()\n",
    "        val_loss = model.evaluate(val_loader, criterion=criterion)\n",
    "        \n",
    "        if task == 'classification':\n",
    "            if val_loss < best_classification_loss:\n",
    "                best_classification_model = model\n",
    "                best_classification_loss = val_loss\n",
    "                best_classification_combination = params\n",
    "        else:  # task == 'regression'\n",
    "            if val_loss < best_regression_loss:\n",
    "                best_regression_model = model\n",
    "                best_regression_loss = val_loss\n",
    "                best_regression_combination = params\n",
    "\n",
    "best_params = {\n",
    "    \"best_classification_combination\": best_classification_combination,\n",
    "    \"best_regression_combination\": best_regression_combination\n",
    "}\n",
    "\n",
    "params_file_path = os.path.join(plot_save_dir, \"best_model_params.json\")\n",
    "with open(params_file_path, 'w') as json_file:\n",
    "    json.dump(best_params, json_file, indent=4)\n",
    "\n",
    "print(\"Best Classification Model Combination:\", best_classification_combination)\n",
    "print(\"Best Regression Model Combination:\", best_regression_combination)\n",
    "print(f\"Best parameters saved to {params_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_best_params(file_path):\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        params = json.load(json_file)\n",
    "    return params\n",
    "\n",
    "loaded_params = read_best_params(params_file_path)\n",
    "print(\"Loaded Parameters:\", loaded_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_mnist_data(use_label_count=False)\n",
    "\n",
    "train_dataset = MultiMNISTDataset(images['train'], labels['train'], transform=transform)\n",
    "val_dataset = MultiMNISTDataset(images['val'], labels['val'], transform=transform)\n",
    "test_dataset = MultiMNISTDataset(images['test'], labels['test'], transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/1: 100%|██████████| 394/394 [00:07<00:00, 49.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Training Loss: 4.9962\n",
      "Epoch [1/1], Validation Loss: 5.3681\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultiLabelCNN(device=device).to(device)\n",
    "model.fit(train_loader, val_loader, epochs=1, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]) tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64)\n",
      "Exact Match Accuracy: 2.43%\n",
      "Hamming Accuracy: 89.70%\n"
     ]
    }
   ],
   "source": [
    "def exact_match_accuracy(preds, labels):\n",
    "    \"\"\"Calculates the exact match accuracy between predictions and labels.\"\"\"\n",
    "    return (preds == labels).all(dim=1).float().mean().item()\n",
    "\n",
    "def hamming_accuracy(preds, labels):\n",
    "    \"\"\"Calculates the Hamming accuracy for multi-label predictions.\"\"\"\n",
    "    return (preds == labels).float().mean().item()\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    preds = model.predict(test_loader)\n",
    "    labels = torch.cat([labels for _, labels in test_loader])\n",
    "    print(preds[12], labels[12])\n",
    "\n",
    "    exact_acc = exact_match_accuracy(preds, labels)\n",
    "    hamming_acc = hamming_accuracy(preds, labels)\n",
    "    print(f\"Exact Match Accuracy: {exact_acc * 100:.2f}%\")\n",
    "    print(f\"Hamming Accuracy: {hamming_acc * 100:.2f}%\")\n",
    "\n",
    "evaluate_model(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
