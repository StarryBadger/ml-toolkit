{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "# from torch.utils.data import Dataset\n",
    "# from torch.utils.data import DataLoader\n",
    "sys.path.append(os.path.abspath(os.path.join('..', '..', 'models', 'AutoEncoders')))\n",
    "# from cnn_autoencoder import CNNAutoencoder\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from tqdm import tqdm\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../../data/interim/4/fashion_mnist'\n",
    "train_path = f'{base_path}/fashion-mnist_train.csv'\n",
    "val_path = f'{base_path}/fashion-mnist_val.csv'\n",
    "test_path = f'{base_path}/fashion-mnist_test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Fashion MNIST dataset from kaggle. Split the training data into\n",
    "train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path = '../../data/external/fashion_mnist'\n",
    "# train_path = f'{base_path}/fashion-mnist_train.csv'\n",
    "# test_path = f'{base_path}/fashion-mnist_test.csv'\n",
    "# output_path = '../../data/interim/4/fashion_mnist'\n",
    "\n",
    "# train_df = pd.read_csv(train_path)\n",
    "# test_df = pd.read_csv(test_path)\n",
    "\n",
    "# train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# train_df_split = train_df[:50000]\n",
    "# val_df_split = train_df[50000:]\n",
    "\n",
    "# train_df_split.to_csv(f'{output_path}/fashion-mnist_train.csv', index=False)\n",
    "# val_df_split.to_csv(f'{output_path}/fashion-mnist_val.csv', index=False)\n",
    "# test_df.to_csv(f'{output_path}/fashion-mnist_test.csv', index=False)\n",
    "\n",
    "# print(\"Data shuffled, split, and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 10 \n",
    "fig, axes = plt.subplots(num_labels, 5, figsize=(10, num_labels * 2.5))\n",
    "\n",
    "for label in range(num_labels):\n",
    "    images = train_df[train_df.iloc[:, 0] == label].iloc[:, 1:].values[:5] \n",
    "    for j, image in enumerate(images):\n",
    "        image_reshaped = image.reshape(28, 28)\n",
    "        axes[label, j].imshow(image_reshaped, cmap='gray')\n",
    "        axes[label, j].set_title(f'Label: {label}')\n",
    "        axes[label, j].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to list the clothing type represented by each class (donâ€™t google this,\n",
    "try it out yourself)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. **T-shirt**\n",
    "\n",
    "1. **Trousers**\n",
    "\n",
    "2. **Sweatesr**: This looks like warm clothing, mostly without a zipper.\n",
    "\n",
    "3. **Dress**\n",
    "\n",
    "4. **Jackets**: This looks like warm clothing, with what seems like zippers in most cases.\n",
    "\n",
    "5. **Sandals**: The images shows open footwear, like heels and wedges.\n",
    "\n",
    "6. **Shirts/Polos**: This looks like a collared shirt with buttons down the front.\n",
    "\n",
    "7. **Sneakers**: These appear to be athletic shoes.\n",
    "\n",
    "8. **Bag**: These resemble purses or handbags.\n",
    "\n",
    "9. **Boots**: These look like footwear but are higher than the sneakers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the CnnAutoencoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "def load_fashion_data():\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    val_df = pd.read_csv(val_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    # Extract features and labels as numpy arrays\n",
    "    X_train = train_df.iloc[:, 1:].values.astype('float32') / 255.0  # Scale to [0, 1]\n",
    "    y_train = train_df.iloc[:, 0].values  # Labels\n",
    "    \n",
    "    X_val = val_df.iloc[:, 1:].values.astype('float32') / 255.0  # Scale to [0, 1]\n",
    "    y_val = val_df.iloc[:, 0].values  # Labels\n",
    "    \n",
    "    X_test = test_df.iloc[:, 1:].values.astype('float32') / 255.0  # Scale to [0, 1]\n",
    "    y_test = test_df.iloc[:, 0].values  # Labels (if available)\n",
    "    \n",
    "    return (train_df, val_df, test_df), (X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "\n",
    "class MultiMNISTDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.dataframe.iloc[idx, 0]\n",
    "        image = self.dataframe.iloc[idx, 1:].values.astype('uint8').reshape(28, 28)  # Convert to 2D array\n",
    "        image = Image.fromarray(image)  # Convert to PIL Image\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "(dataframes, (X_train, y_train, X_val, y_val, X_test, y_test)) = load_fashion_data()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "])\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = MultiMNISTDataset(dataframes[0], transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = MultiMNISTDataset(dataframes[1], transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = MultiMNISTDataset(dataframes[2], transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=test_dataset[0]\n",
    "image, label = sample\n",
    "print(image[0][10][10])\n",
    "print(X_test[0][28*10+10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNAutoencoder(device=device).to(device)\n",
    "model.fit(train_loader, num_epochs=10, learning_rate=0.001)\n",
    "model.evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_reconstructed_images(model, data_loader, num_images=10):\n",
    "    model.eval()\n",
    "    \n",
    "    data_iter = iter(data_loader)\n",
    "    images, _ = next(data_iter)\n",
    "    images = images[:num_images]  \n",
    "    images = images.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        reconstructed = model(images)\n",
    "\n",
    "    images = images.cpu()\n",
    "    reconstructed = reconstructed.cpu()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_images, figsize=(20, 4))\n",
    "    for i in range(num_images):\n",
    "        axes[0, i].imshow(images[i].squeeze(), cmap='gray')\n",
    "        axes[0, i].set_title(\"Original\")\n",
    "        axes[0, i].axis('off')\n",
    "        axes[1, i].imshow(reconstructed[i].squeeze(), cmap='gray')\n",
    "        axes[1, i].set_title(\"Reconstructed\")\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "show_reconstructed_images(model, test_loader, num_images=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_latent_space(model, data_loader, num_samples=1000):\n",
    "    model.eval()\n",
    "    latent_representations = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x = x.to(device)\n",
    "            encoded = model.encode(x)\n",
    "            latent_representations.append(encoded.view(encoded.size(0), -1).cpu().numpy())\n",
    "            labels.extend(y.cpu().numpy())\n",
    "            if len(labels) >= num_samples:\n",
    "                break\n",
    "    \n",
    "    latent_representations = np.concatenate(latent_representations)[:num_samples]\n",
    "    labels = np.array(labels)[:num_samples]\n",
    "    \n",
    "    pca_2d = PCA(n_components=2)\n",
    "    latent_2d = pca_2d.fit_transform(latent_representations)\n",
    "    \n",
    "    pca_3d = PCA(n_components=3)\n",
    "    latent_3d = pca_3d.fit_transform(latent_representations)\n",
    "\n",
    "    # 2D plot\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "    plt.colorbar(scatter, label='Class Label')\n",
    "    plt.xlabel(\"PC 1\")\n",
    "    plt.ylabel(\"PC 2\")\n",
    "    plt.title(\"2D PCA of Latent Space\")\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    scatter = ax.scatter(latent_3d[:, 0], latent_3d[:, 1], latent_3d[:, 2], c=labels, cmap='tab10', alpha=0.7)\n",
    "    fig.colorbar(scatter, ax=ax, label='Class Label')\n",
    "    ax.set_xlabel(\"PC 1\")\n",
    "    ax.set_ylabel(\"PC 2\")\n",
    "    ax.set_zlabel(\"PC 3\")\n",
    "    plt.title(\"3D PCA of Latent Space\")\n",
    "    plt.show()\n",
    "\n",
    "plot_latent_space(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AutoEncoders import AutoEncoder\n",
    "\n",
    "\n",
    "input_size = 28*28\n",
    "latent_size = 75\n",
    "encoder_layers = []  \n",
    "decoder_layers = []  \n",
    "learning_rate = 0.08\n",
    "max_epochs = 500\n",
    "batch_size = 32\n",
    "\n",
    "autoencoder = AutoEncoder(\n",
    "    input_size=input_size,\n",
    "    latent_size=latent_size,\n",
    "    encoder_layers=encoder_layers,\n",
    "    decoder_layers=decoder_layers,\n",
    "    learning_rate=learning_rate,\n",
    "    optimizer='mbgd'\n",
    ")\n",
    "print(X_train.shape)\n",
    "autoencoder.fit(X_train, max_epochs=max_epochs, batch_size=batch_size)\n",
    "X_reconstructed = autoencoder.reconstruct(X_train)\n",
    "\n",
    "\n",
    "def visualize_reconstruction(autoencoder, X_test, n_images=5):\n",
    "    X_reconstructed = autoencoder.reconstruct(X_test[:n_images])\n",
    "\n",
    "    fig, axes = plt.subplots(2, n_images, figsize=(15, 4))\n",
    "    for i in range(n_images):\n",
    "        axes[0, i].imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
    "        axes[0, i].set_title(\"Original\")\n",
    "        axes[0, i].axis('off')\n",
    "\n",
    "        axes[1, i].imshow(X_reconstructed[i].reshape(28, 28), cmap='gray')\n",
    "        axes[1, i].set_title(\"Reconstructed\")\n",
    "        axes[1, i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "visualize_reconstruction(autoencoder, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pca_autoencoder import PcaAutoencoder\n",
    "\n",
    "def plot_construction_loss(X_train, max_components=20):\n",
    "    errors = []\n",
    "    for n in range(1, max_components + 1):\n",
    "        pca_autoencoder = PcaAutoencoder(n_components=n)\n",
    "        pca_autoencoder.fit(X_train)\n",
    "        error = pca_autoencoder.reconstruction_error(X_train)\n",
    "        errors.append(error)\n",
    "\n",
    "    plt.plot(range(1, max_components + 1), errors, marker='o')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Reconstruction Error')\n",
    "    plt.title('Reconstruction Error vs Number of Components')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return errors\n",
    "\n",
    "errors = plot_construction_loss(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca_autoencoder = PcaAutoencoder(n_components=75)\n",
    "pca_autoencoder.fit(X_train)\n",
    "\n",
    "def visualize_reconstruction(autoencoder, X_test, n_images=5):\n",
    "    X_reconstructed = autoencoder.forward(X_test[:n_images])\n",
    "\n",
    "    fig, axes = plt.subplots(2, n_images, figsize=(15, 4))\n",
    "    for i in range(n_images):\n",
    "        axes[0, i].imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
    "        axes[0, i].set_title(\"Original\")\n",
    "        axes[0, i].axis('off')\n",
    "\n",
    "        axes[1, i].imshow(X_reconstructed[i].reshape(28, 28), cmap='gray')\n",
    "        axes[1, i].set_title(\"Reconstructed\")\n",
    "        axes[1, i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "visualize_reconstruction(pca_autoencoder, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..', '..', 'models', 'knn')))\n",
    "sys.path.append(os.path.abspath(os.path.join('..', '..', 'performance_measures')))\n",
    "from knn import KNN\n",
    "from classification_metrics import Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca_autoencoder import PcaAutoencoder\n",
    "pca_autoencoder_knn = PcaAutoencoder(n_components=64)\n",
    "pca_autoencoder_knn.fit(X_train)\n",
    "\n",
    "X_train_encoded_pca = pca_autoencoder_knn.encode(X_train)\n",
    "X_test_encoded_pca = pca_autoencoder_knn.encode(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN with PCA: 0.86\n"
     ]
    }
   ],
   "source": [
    "knn_pca = KNN(k=5, distance_metric='cosine')\n",
    "knn_pca.fit(X_train_encoded_pca, y_train)\n",
    "y_pred_pca_1 = knn_pca.predict(X_test_encoded_pca[:5000])\n",
    "y_pred_pca_2 = knn_pca.predict(X_test_encoded_pca[5000:])\n",
    "scores_pca = Metrics(y_test, np.concatenate((y_pred_pca_1,y_pred_pca_2)), task='classification')\n",
    "accuracy_pca = scores_pca.accuracy()\n",
    "print(f\"Accuracy of KNN with PCA: {accuracy_pca:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_autoencoder_knn = AutoEncoder(\n",
    "    input_size=input_size,\n",
    "    latent_size=latent_size,\n",
    "    encoder_layers=encoder_layers,\n",
    "    decoder_layers=decoder_layers,\n",
    "    learning_rate=0.008,\n",
    "    optimizer='mbgd'\n",
    ")\n",
    "print(X_train.shape)\n",
    "autoencoder.fit(X_train, max_epochs=75, batch_size=batch_size)\n",
    "X_train_encoded_mlp = mlp_autoencoder_knn.get_latent(X_train)\n",
    "X_test_encoded_mlp = mlp_autoencoder_knn.get_latent(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_mlp = KNN(k=5, distance_metric='cosine')\n",
    "knn_mlp.fit(X_train_encoded_mlp, y_train)\n",
    "y_pred_mlp = knn_mlp.predict(X_test_encoded_mlp)\n",
    "scores_pca = Metrics(y_test, y_pred_mlp, task='classification')\n",
    "accuracy_pca = scores_pca.accuracy()\n",
    "print(f\"Accuracy of KNN with MLP: {accuracy_pca:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
